<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="distributed-training, AiHub">
    <meta name="description" content="PytorchDataset
next(iter(dataloader))返回一个batch的数据 , 等价于IterableDataset

可以用 pytorch IterableDataset + python generator f">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>distributed-training | AiHub</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>
	
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4517244572130756"
		 crossorigin="anonymous"></script>
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="AiHub" type="application/atom+xml">
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">AiHub</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">AiHub</div>
        <div class="logo-desc">
            
            Fire
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/lee-jet" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/lee-jet" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">distributed-training</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Paper-Reading/">
                                <span class="chip bg-color">Paper Reading</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Paper-Reading/" class="post-category">
                                Paper Reading
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-08-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2023-08-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    32 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ul>
<li><p>next(iter(dataloader))返回一个batch的数据 , 等价于IterableDataset</p>
</li>
<li><p>可以用 pytorch IterableDataset + python generator function(yield) 来解决，这样按需读取数据，常用的 dataset 是一次全部加载。 IterableDataset 因为按需读取，就没法用 len, index 等功能</p>
</li>
<li><p>Pytorch提供了两种思路去构建<code>Dataset</code>:</p>
<ul>
<li><p><strong>Map</strong>式数据集: 将整个数据集读取到内存中，通过index<strong>映射</strong>的方式读取对应的数据，优点速度快，缺点占用内存，大的数据集是无法使用。</p>
</li>
<li><p><strong>Iterable</strong>式数据集：无需将整个数据集读取到内存中，通过覆盖写<code>iter</code>迭代器的方式实现<strong>流</strong>的形式输入数据。<strong>无需满足内存大</strong>于整个数据集，也无需知道全部数据的大小。</p>
</li>
</ul>
</li>
</ul>
<p>​        简单来说，如果数据集较小时推荐尽量使用<strong>Map</strong>式Dataset，数据量过大、数据量未知、训练内存无法满足时只能使用<code>Iterable</code>式构建Dataset。</p>
<ul>
<li>iterable Dataset 在分布式训练</li>
</ul>
<p>​        在分布式时训练中数据并行的时，每块GPU都有一个独立的model和独立的进程(DDP模式)去训练完整数据的子集，在Pytorch中的DDP模式是通过<code>DistributedSampler()</code>去实现在分布式并行训练时每个模型读取是整个数据集上不同部分，从而避免训练时取数据发生重复。比较坑的是：DistributedSampler()主要通过切片的方式将整个数据集分成独立的子集。Map 式 Dataset无需再多写代码直接可以适用。而iterable式 Dataset是无法切片。</p>
<p>​        注意，如果它是一个带有一些随机性的 <code>torch.utils.data.IterableDataset</code> ，并且你是以分布式方式进行训练的，你的 <code>iterable dataset</code> 要么使用一个内部的 <code>attribute generator</code> ，该<code>generator</code> 是一个 <code>torch.Generator</code> 用于随机化，且在所有进程上必须是相同的（并且 <code>Trainer</code> 将在每个 <code>epoch</code> 手动设置该 <code>generator</code> 的种子）；要么有一个 <code>set_epoch()</code> 方法，在该方法内部设置所用随机数生成器的种子。</p>
<ul>
<li>pytorch DistributedSampler的实现</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Iterator<span class="token punctuation">[</span>T_co<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>shuffle<span class="token punctuation">:</span>
        <span class="token comment"># deterministically shuffle based on epoch and seed</span>
        g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span>
        g<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>self<span class="token punctuation">.</span>seed <span class="token operator">+</span> self<span class="token punctuation">.</span>epoch<span class="token punctuation">)</span>
        indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        indices <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> IterableDataset

<span class="token keyword">class</span> <span class="token class-name">IterableDataset</span><span class="token punctuation">(</span>IterableDataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> file_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>file_path <span class="token operator">=</span> file_path

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>file_path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                items <span class="token operator">=</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\t"</span><span class="token punctuation">)</span>
                <span class="token comment"># process data </span>
                <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
                <span class="token keyword">yield</span> <span class="token punctuation">&#123;</span><span class="token string">"input1"</span><span class="token punctuation">:</span> input1<span class="token punctuation">,</span> <span class="token string">"input2"</span><span class="token punctuation">:</span> input2<span class="token punctuation">,</span> <span class="token string">"input3"</span><span class="token punctuation">:</span> input3<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p>数据并行：因为求导以及加和都是线性的，数据并行在数学上也有效</p>
<ul>
<li>一个dataset划分为若干个batch</li>
<li>每个GPU复制一份模型</li>
<li>将每个batch平均分配到所有GPU上并行运算</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/image-20230817103103621.png" alt="image-20230817103103621"></p>
<h3 id="DP"><a href="#DP" class="headerlink" title="DP"></a>DP</h3><ul>
<li>使用</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li><p>DataParallel(DP)</p>
</li>
<li><p>适用单机，不适用多机</p>
</li>
<li><p>优点：一行代码即可</p>
</li>
<li><p>缺点</p>
<ul>
<li>负载不均衡，即存在主次模型（主模型需要整合其它次模型的梯度进行参数更新），主模型负载更大；</li>
<li>通信开销大</li>
</ul>
<p>  <img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/image-20230817104337458.png" alt="image-20230817104337458"></p>
<ul>
<li>单进程多线程</li>
<li>Global Interpreter Lock (GIL)全局解释器锁：一个 Python 进程只能利用一个 CPU kernel，即单核多线程并发时，只能执行一个线程。考虑多核，多核多线程可能出现线程颠簸 (thrashing) 造成资源浪费，所以 Python 想要利用多核最好是多进程</li>
</ul>
</li>
</ul>
<ul>
<li>过程（ 比如device[0]为主模型，其它为次模型）<ul>
<li>过程一（图中红色部分）：各卡分别计算损失和梯度</li>
<li>过程二（图中蓝色部分）：所有梯度整合到 device[0]</li>
<li>过程三（图中绿色部分）：device[0] 进行参数更新，其他卡拉取 device[0] 的参数进行更新</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/image-20230817103746077.png" alt="image-20230817103746077"></p>
<ul>
<li>采用PS架构（Parameter Server）</li>
<li>DP源码</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DataParallel</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DataParallel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># 检查是否有可用的 GPU</span>
        device_type <span class="token operator">=</span> _get_available_device_type<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> device_type <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
            self<span class="token punctuation">.</span>device_ids <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">return</span>
                <span class="token comment"># 默认使用所有可见的 GPU</span>
        <span class="token keyword">if</span> device_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            device_ids <span class="token operator">=</span> _get_all_device_indices<span class="token punctuation">(</span><span class="token punctuation">)</span>

                <span class="token comment"># 默认 server 是 device_ids 列表上第一个</span>
        <span class="token keyword">if</span> output_device <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            output_device <span class="token operator">=</span> device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>device_ids <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> _get_device_index<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output_device <span class="token operator">=</span> _get_device_index<span class="token punctuation">(</span>output_device<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>src_device_obj <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>device_type<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># 检查负载是否平衡， 不平衡（指内存或者处理器 max/min > 0.75 会有警告）</span>
        _check_balance<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>

        <span class="token comment"># 单卡</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>src_device_obj<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token comment"># 没 GPU 可用</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>

                <span class="token comment"># 运行前 GPU device_ids[0] （即我们的 server ）上必须有 parallelized module 的parameters 和 buffers</span>
        <span class="token comment"># 因为 DP 保证 GPU device_ids[0] 和 base parallelized module 共享存储</span>
        <span class="token comment"># 所以在device[0] 上的 in-place 更新也会被保留下来，其他的则不会</span>

        <span class="token keyword">for</span> t <span class="token keyword">in</span> chain<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>buffers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> t<span class="token punctuation">.</span>device <span class="token operator">!=</span> self<span class="token punctuation">.</span>src_device_obj<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> RuntimeError<span class="token punctuation">(</span><span class="token string">"module must have its parameters and buffers "</span>
                                   <span class="token string">"on device &#123;&#125; (device_ids[0]) but found one of "</span>
                                   <span class="token string">"them on device: &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>src_device_obj<span class="token punctuation">,</span> t<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token comment"># nice 现在 device[0] 上已经有了 module 和 input， 接下来我们就要开始 PS 算法了</span>
        <span class="token comment"># 可以开始看正文了</span>

        inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>

        <span class="token comment"># 如果仅有单卡可用，直接单卡计算，不用并行</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        replicas <span class="token operator">=</span> self<span class="token punctuation">.</span>replicate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>parallel_apply<span class="token punctuation">(</span>replicas<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_device<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">replicate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> device_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> replicate<span class="token punctuation">(</span>module<span class="token punctuation">,</span> device_ids<span class="token punctuation">,</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>is_grad_enabled<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">scatter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> device_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> scatter_kwargs<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> device_ids<span class="token punctuation">,</span> dim<span class="token operator">=</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parallel_apply</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> replicas<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> parallel_apply<span class="token punctuation">(</span>replicas<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token builtin">len</span><span class="token punctuation">(</span>replicas<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">gather</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outputs<span class="token punctuation">,</span> output_device<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> gather<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> output_device<span class="token punctuation">,</span> dim<span class="token operator">=</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="DDP"><a href="#DDP" class="headerlink" title="DDP"></a>DDP</h3><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ul>
<li><p>group: 进程组，一般就需要一个默认的</p>
</li>
<li><p>world size: 所有的进程数量</p>
</li>
<li><p>rank: 全局的进程id</p>
</li>
<li><p>local rank：某个节点上的进程id</p>
</li>
<li><p>local_word_size: 某个节点上的进程数 (相对比较少见)</p>
<p>​    </p>
</li>
</ul>
<p>假设所有进程数即 world_size为W，每个节点上的进程数即local_world_size为L，则每个进程上的两个ID：</p>
<ul>
<li>rank的取值范围：[0, W-1]，rank=0的进程为主进程，会负责一些同步分发的工作</li>
<li>local_rank的取值：[0, L-1]</li>
<li>假定有2个<strong>机器或者节点</strong>，每个机器上有4块GPU。图中一共有4个进程，即world_size=4，那这样每个进程占用两块GPU，其中rank就是[0,1,2,3]，每个节点的local_rank就是[0,1]了（<strong>4个进程，2个节点，平均每个节点2个进程</strong>），其中local_world_size 也就是2。 这里需要注意的是，<strong>local_rank是隐式参数，即torch自动分配的</strong>。比如local_rank可以通过自动注入命令行参数或者环境变量来获得)** 。</li>
<li>通常–nproc_per_node等于显卡数，也就是一个显卡分配一个进程</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> argparse<span class="token punctuation">,</span> os

parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--local_rank"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span>ine<span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">"nccl"</span><span class="token punctuation">)</span>
rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
local_rank_arg <span class="token operator">=</span> args<span class="token punctuation">.</span>local_rank               <span class="token comment"># 命令行形式ARGS形式</span>
local_rank_env <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 在利用env初始ENV环境变量形式</span>
local_world_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_WORLD_SIZE'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>rank<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>local_rank_arg<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>local_rank_env<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>local_world_size<span class="token operator">=</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ python3 <span class="token parameter variable">-m</span> torch.distributed.launch <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> test.py
<span class="token assign-left variable">rank</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_arg</span><span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_env</span><span class="token operator">=</span><span class="token number">2</span>, <span class="token assign-left variable">local_world_size</span><span class="token operator">=</span><span class="token number">4</span>
<span class="token assign-left variable">rank</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_arg</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_env</span><span class="token operator">=</span><span class="token number">0</span>, <span class="token assign-left variable">local_world_size</span><span class="token operator">=</span><span class="token number">4</span>
<span class="token assign-left variable">rank</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_arg</span><span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_env</span><span class="token operator">=</span><span class="token number">3</span>, <span class="token assign-left variable">local_world_size</span><span class="token operator">=</span><span class="token number">4</span>
<span class="token assign-left variable">rank</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_arg</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span> <span class="token assign-left variable">local_rank_env</span><span class="token operator">=</span><span class="token number">1</span>, <span class="token assign-left variable">local_world_size</span><span class="token operator">=</span><span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> world_size<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> store<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p><code>backend</code></p>
<ul>
<li>torch提供了<code>NCCL, GLOO,MPI</code>三种可用的后端</li>
<li>CPU的分布式训练选择<code>GLOO</code>, GPU的分布式训练就用<code>NCCL</code>即可</li>
</ul>
<p><code>init_method</code></p>
<ul>
<li>显式指定<code>init_method</code>，可以是TCP连接、File共享文件系统、ENV环境变量三种方式</li>
<li>显式指定<code>store</code>，同时指定world_size 和 rank参数。这里的store是一种分布式中核心的key-value存储，用于不同的进程间共享信息。</li>
</ul>
<p>这两种方法是互斥的，其实本质上第一种方式是对第二种的一个更高的封装，最后都要落到store上进行实现。如果这两种方法都没有使用，默认使用<code>init_method=&#39;env&#39;</code>的方式来初始化。</p>
<p><code>三种init_method</code>：</p>
<ul>
<li><code>init_method=&#39;tcp://ip:port&#39;</code>： 通过指定rank 0（即：MASTER进程）的IP和端口，各个进程进行信息交换。 需指定 rank 和 world_size 这两个参数。</li>
<li><code>init_method=&#39;file://path&#39;</code>：通过所有进程都可以访问共享文件系统来进行信息共享。需要指定rank和world_size参数。</li>
<li><code>init_method=env://</code>：从环境变量中读取分布式的信息(os.environ)，主要包括 <code>MASTER_ADDR, MASTER_PORT, RANK, WORLD_SIZE</code>。 其中，rank和world_size可以选择手动指定，否则从环境变量读取。</li>
</ul>
<h4 id="运行方法"><a href="#运行方法" class="headerlink" title="运行方法"></a>运行方法</h4><ol>
<li><code>torch.multiprocessing</code>（python的<code>multiprocessing</code>的封装类)</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">mp<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>fn<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nprocs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> join<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> daemon<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>multiprocessing <span class="token keyword">as</span> mp

<span class="token keyword">def</span> <span class="token function">fn</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> ws<span class="token punctuation">,</span> nums<span class="token punctuation">)</span><span class="token punctuation">:</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">'nccl'</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token string">'tcp://127.0.0.1:28765'</span><span class="token punctuation">,</span>
                            rank<span class="token operator">=</span>rank<span class="token punctuation">,</span> world_size<span class="token operator">=</span>ws<span class="token punctuation">)</span>
    rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"rank = </span><span class="token interpolation"><span class="token punctuation">&#123;</span>rank<span class="token punctuation">&#125;</span></span><span class="token string"> is initialized"</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>nums<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    ws <span class="token operator">=</span> <span class="token number">2</span>
    mp<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>fn<span class="token punctuation">,</span> nprocs<span class="token operator">=</span>ws<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>ws<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">rank <span class="token operator">=</span> <span class="token number">0</span> is initialized
rank <span class="token operator">=</span> <span class="token number">1</span> is initialized
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span><span class="token punctuation">]</span>, <span class="token assign-left variable">device</span><span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span>, <span class="token number">2</span>, <span class="token number">3</span>, <span class="token number">4</span><span class="token punctuation">]</span>, <span class="token assign-left variable">device</span><span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p>​    2. launch</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ python3 <span class="token parameter variable">-m</span> torch.distributed.launch --配置 train.py --args参数

<span class="token comment"># 通过外部命令运行 </span>
<span class="token comment"># 通过CUDA_VISIBLE_DEVICES控制可见的卡数</span>
<span class="token comment"># 通过--nproc_per_node确定使用多少卡</span>
<span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token string">"0,1,2,3"</span> python <span class="token parameter variable">-m</span> torch.distributed.run <span class="token parameter variable">--nproc_per_node</span> <span class="token number">4</span> train.py 

常用配置有:
--nnodes: 使用的机器数量，单机的话，就默认是1了
--nproc_per_node: 单机的进程数，即单机的worldsize
--master_addr/port: 使用的主进程rank0的地址和端口
--node_rank: 当前的进程rank

在单机情况下， 只有--nproc_per_node 是必须指定的，--master_addr/port和node_rank都是可以由launch通过环境自动配置<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ol start="3">
<li>run</li>
</ol>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ torchrun <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> train.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>命令<code>torchrun</code>来代替<code>torch.distributed.launch</code></p>
<ul>
<li>完全使用环境变量配置各类参数，如<code>RANK,LOCAL_RANK, WORLD_SIZE</code>等，尤其是<code>local_rank</code>不再支持用命令行隐式传递的方式</li>
<li>能够更加优雅的处理某个worker失败的情况，重启worker。需要代码中有<code>load_checkpoint(path)</code>和<code>save_checkpoint(path)</code> 这样有worker失败的话，可以通过load最新的模型，重启所有的worker接着训练</li>
<li>训练的节点数目可以弹性变化</li>
</ul>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p><strong>步骤</strong></p>
<ul>
<li><p>初始化进程组 <code>dist.init_process_group</code></p>
</li>
<li><p>设置分布式采样器 <code>DistributedSampler</code></p>
</li>
<li><p>使用<code>DistributedDataParallel</code>封装模型</p>
</li>
<li><p>使用<code>torchrun</code> 或者 <code>mp.spawn</code> 启动分布式训练</p>
<p>  <img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/image-20230817150712701.png" alt="image-20230817150712701"></p>
</li>
</ul>
<p><strong>1 分布式训练数据加载</strong></p>
<ul>
<li>Dataloader需要把所有数据分成N份(N为worldsize), 并能正确的分发到不同的进程中，每个进程可以拿到一个数据的子集，不重叠，不交叉。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>
				num_replicas<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ul>
<li>dataset: 需要加载的完整数据集</li>
<li>num_replicas： 把数据集分成多少份，默认是当前dist的world_size</li>
<li>rank: 当前进程的id，默认从dist的rank</li>
<li>shuffle：是否打乱</li>
<li>drop_last: 如果数据长度不能被world_size整除，可以考虑是否将剩下的扔掉</li>
<li>seed：随机数种子。这里需要注意，从源码中可以看出，真正的种子其实是 <code>self.seed+self.epoch</code> 这样的好处是，不同的epoch每个进程拿到的数据是不一样，因此需要在每个epoch开始前设置下：<code>sampler.set_epoch(epoch)</code></li>
</ul>
<p>​    其实Sampler的实现也很简单，核心代码就一句：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">indices<span class="token punctuation">[</span>self.rank: self.total_size: self.num_replicas<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>假设4卡12条数据的话，rank=0,1,2,3, num_replicas=4, 那么每个卡取的数据索引就是：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">rank0: <span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">4</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">;</span> rank1: <span class="token punctuation">[</span><span class="token number">1</span> <span class="token number">5</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">;</span> rank2: <span class="token punctuation">[</span><span class="token number">2</span> <span class="token number">6</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">;</span> rank3: <span class="token punctuation">[</span><span class="token number">3</span> <span class="token number">7</span> <span class="token number">11</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>保证不重复不交叉。这样在分布式训练的时候，只需要给Dataloader指定DistributedSampler即可，简单示例如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> sampler<span class="token operator">=</span>sampler<span class="token punctuation">)</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>start_epoch<span class="token punctuation">,</span> n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
  sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span> <span class="token comment"># 设置epoch 更新种子</span>
  train<span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>if args.local_rank == -1 表示关闭分布式</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">train_sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span> <span class="token keyword">if</span> args<span class="token punctuation">.</span>local_rank <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token keyword">else</span> DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<p><strong>2 模型的分布式训练封装</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>
model <span class="token operator">=</span> Model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 要调用model内的函数或者属性. model.module.xxxx</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>3 模型保存与加载</strong>        </p>
<ul>
<li>训练时候保存模型，只保存rank=0主进程模型，不需要dist.barrior()， all_reduce 操作保证了同步性</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 后面正常训练代码</span>
optimizer <span class="token operator">=</span> xxx
<span class="token keyword">for</span> epoch<span class="token punctuation">:</span>
  train_sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
  <span class="token keyword">for</span> data <span class="token keyword">in</span> Dataloader<span class="token punctuation">:</span>
      model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
      xxx
    <span class="token comment"># 训练完成 只需要保存rank 0上的即可</span>
    <span class="token comment"># 不需要dist.barrior()， all_reduce 操作保证了同步性</span>
  <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
     torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CHECKPOINT_PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 保存的是参数，不需要DDP包裹</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ul>
<li>推理时候加载模型，需要barrier()其他保证rank 0保存完成。</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
CHECKPOINT_PATH <span class="token operator">=</span><span class="token string">"./model.checkpoint"</span>
<span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span> <span class="token comment"># 主进程</span>
  torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>ddp_model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CHECKPOINT_PATH<span class="token punctuation">)</span>
<span class="token comment"># barrier()其他保证rank 0保存完成</span>
dist<span class="token punctuation">.</span>barrier<span class="token punctuation">(</span><span class="token punctuation">)</span>
map_location <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"cuda:0"</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f"cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>local_rank<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">&#125;</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>CHECKPOINT_PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span>map_location<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>这样在多卡训练时，每个进程有一个model副本和optimizer，使用自己的数据进行训练，之后反向传播计算完梯度的时候，所有进程的梯度会进行all-reduce操作进行同步，进而保证每个卡上的模型更新梯度是一样的，模型参数也是一致的。</p>
<p>​        这里有一个需要注意的地方，在save和load模型时候，为了减小所有进程同时读写磁盘，一般处理方法是以主进程为主，rank0先save模型，在map到其他进程。这样的另外一个好处，在最开始训练时，模型随机初始化之后，保证了所有进程的模型参数保持一致。</p>
<p><strong>4 损失函数</strong></p>
<ul>
<li>loss.backward() 不变</li>
<li>如果计算loss数值，用下面的all_reduce。或者也可以不用</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 仍然可以直接调用模型的train()方法</span>
<span class="token comment"># 但是假如要调用其他你自己写的方法，就得model.module.func()</span>
model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> data <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
    loss <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 这个操作自动同步梯度</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 但是仍然需要累加得到所有进程loss的值的和</span>
    dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> op<span class="token operator">=</span>dist<span class="token punctuation">.</span>ReduceOp<span class="token punctuation">.</span>SUM<span class="token punctuation">)</span>
    <span class="token comment"># 然后除以并行数，就是这个batch的loss值了</span>
    loss <span class="token operator">/=</span> world_size<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="模型推理"><a href="#模型推理" class="headerlink" title="模型推理"></a>模型推理</h4><p>一般需要先所有进程的输出结果进行gather，再进行指标的计算，两个常用的函数:</p>
<ul>
<li><code>dist.all_gather(tensor_list, tensor)</code> : 将所有进程的tensor进行收集并拼接成新的tensorlist返回，比如:<code>dist.all_reduce(tensor, op)</code> 这是对<code>tensor</code>的in-place的操作, 对所有进程的某个tensor进行合并操作，op可以是求和等：</li>
<li>拿到所有进程中模型的输出，最后统一计算指标</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pred_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> data <span class="token keyword">in</span> Dataloader<span class="token punctuation">:</span>
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
    batch_pred <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>label<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">]</span>
    dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>batch_pred<span class="token punctuation">,</span> pred<span class="token punctuation">)</span>
    pred_list<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>batch_pred<span class="token punctuation">)</span>
pred_list <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>pred_list<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token comment"># 所有进程pred_list是一致的，保存所有数据模型预测的值</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_loader<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    preds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> data <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
            inputs<span class="token punctuation">,</span> truth <span class="token operator">=</span> data
            inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            truth <span class="token operator">=</span> truth<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'logits'</span><span class="token punctuation">]</span>
            predict <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

            cur_preds <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>predict<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
            cur_truth <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>truth<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>cur_preds<span class="token punctuation">,</span> predict<span class="token punctuation">)</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>cur_truth<span class="token punctuation">,</span> truth<span class="token punctuation">)</span>

            preds<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>cur_preds<span class="token punctuation">)</span>
            labels<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>cur_truth<span class="token punctuation">)</span>

    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    predict <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    correct <span class="token operator">=</span> <span class="token punctuation">(</span>predict <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> correct <span class="token operator">*</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>predict<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4><ol>
<li>要把模型和数据放在进程对应的那张卡上</li>
<li>要使用Sampler来分发训练数据，并且shuffle不设置在Dataloder中而是Sampler中，每个epoch还需要调用Sampler的<code>set_epoch()</code>方法。(需要set_epoch来使用sampleer中shuffle，否则不是随机的)</li>
<li>训练和验证区分较大，验证一般在主进程中进行一次验证即可，不需要sampler，操作和单卡一样，之后将数据同步给其他进程。</li>
<li>在多卡时要调用模型的其他方法或者使用单卡的模式，需要用<code>model.module</code>来获得原始模型，同样保存参数时也保存的是<code>model.module</code>的参数而不是DDP包裹的。</li>
</ol>
<h4 id="使用总结"><a href="#使用总结" class="headerlink" title="使用总结"></a>使用总结</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> argparse
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP

parser <span class="token operator">=</span> argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--save_dir"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token string">''</span><span class="token punctuation">)</span>
parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--local_rank"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">"--world_size"</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
args <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 初始化后端 建议NCCL</span>

<span class="token comment"># world_size 指的是总的并行进程数目</span>
<span class="token comment"># 比如16张卡单卡单进程 就是 16</span>
<span class="token comment"># 但是如果是8卡单进程 就是 1</span>
<span class="token comment"># 等到连接的进程数等于world_size，程序才会继续运行</span>
torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span>
                                         world_size<span class="token operator">=</span>ws<span class="token punctuation">,</span>
                                         init_method<span class="token operator">=</span><span class="token string">'env://'</span><span class="token punctuation">)</span>

torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token comment"># train dataset</span>
<span class="token comment"># train_sampler</span>
<span class="token comment"># train_loader</span>

my_trainset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 新增1：使用DistributedSampler，DDP帮我们把细节都封装起来了。用，就完事儿！</span>
<span class="token comment">#       sampler的原理，后面也会介绍。</span>
train_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>my_trainset<span class="token punctuation">)</span>
<span class="token comment"># 需要注意的是，这里的batch_size指的是每个进程下的batch_size。也就是说，总batch_size是这里的batch_size再乘以并行数(world_size)。</span>
trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>my_trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>


<span class="token comment"># 初始化 DDP，这里我们通过规定 device_id 用了单卡单进程</span>
<span class="token comment"># 实际上根据我们前面对 parallel_apply 的解读，DDP 也支持一个进程控制多个线程利用多卡</span>
model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span>
            device_ids<span class="token operator">=</span><span class="token punctuation">[</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">,</span>
            output_device<span class="token operator">=</span>args<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    trainloader<span class="token punctuation">.</span>sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
    <span class="token comment"># 后面这部分，则与原来完全一致了。</span>
    <span class="token keyword">for</span> data<span class="token punctuation">,</span> label <span class="token keyword">in</span> trainloader<span class="token punctuation">:</span>
        prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>ddp_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
<span class="token comment"># 保存模型 </span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
  torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
             <span class="token string">'results/%s/model.pth'</span> <span class="token operator">%</span> args<span class="token punctuation">.</span>save_dir<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><p>区别</p>
<ul>
<li>多进程<br>  和 DP 不同， DDP 采用多进程，最推荐的做法是每张卡一个进程从而避免上一节所说单进程带来的影响。前文也提到了 DP 和 DDP 共用一个 parallel_apply 函数，所以 DDP 同样支持单进程多线程多卡操作，自然也支持多进程多线程，不过需要注意一下 world_size。</li>
<li>通信效率<br>  DP 的通信成本随着 GPU 数量线性增长，而 DDP 支持 Ring AllReduce，其通信成本是恒定的，与 GPU 数量无关。</li>
<li>同步参数<br>  DP 通过收集梯度到 device[0]，在device[0] 更新参数，然后其他设备复制 device[0] 的参数实现各个模型同步；<br>  DDP 通过保证初始状态相同并且改变量也相同（指同步梯度） ，保证模型同步。</li>
</ul>
</li>
<li><p>源码</p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DistributedDataParallel</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>       
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> broadcast_buffers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 process_group<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>  
                 bucket_cap_mb<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span>       
                 find_unused_parameters<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>       
                 check_reduction<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>      
                 gradient_as_bucket_view<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span>DistributedDataParallel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">assert</span> <span class="token builtin">any</span><span class="token punctuation">(</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>requires_grad <span class="token keyword">for</span> p <span class="token keyword">in</span> module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>
            <span class="token string">"DistributedDataParallel is not needed when a module "</span>
            <span class="token string">"doesn't have any parameter that requires a gradient."</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>is_multi_device_module <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>p<span class="token punctuation">.</span>device <span class="token keyword">for</span> p <span class="token keyword">in</span> module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span>
        distinct_device_types <span class="token operator">=</span> <span class="token punctuation">&#123;</span>p<span class="token punctuation">.</span>device<span class="token punctuation">.</span><span class="token builtin">type</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>distinct_device_types<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>
            <span class="token string">"DistributedDataParallel's input module must be on "</span>
            <span class="token string">"the same type of devices, but input module parameters locate in &#123;&#125;."</span>
        <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>distinct_device_types<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>device_type <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>distinct_device_types<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>device_type <span class="token operator">==</span> <span class="token string">"cpu"</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>is_multi_device_module<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> <span class="token keyword">not</span> device_ids <span class="token keyword">and</span> <span class="token keyword">not</span> output_device<span class="token punctuation">,</span> <span class="token punctuation">(</span>
                <span class="token string">"DistributedDataParallel device_ids and output_device arguments "</span>
                <span class="token string">"only work with single-device GPU modules, but got "</span>
                <span class="token string">"device_ids &#123;&#125;, output_device &#123;&#125;, and module parameters &#123;&#125;."</span>
            <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>device_ids<span class="token punctuation">,</span> output_device<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>p<span class="token punctuation">.</span>device <span class="token keyword">for</span> p <span class="token keyword">in</span> module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>

            self<span class="token punctuation">.</span>device_ids <span class="token operator">=</span> <span class="token boolean">None</span>
            self<span class="token punctuation">.</span>output_device <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># Use all devices by default for single-device GPU modules</span>
            <span class="token keyword">if</span> device_ids <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                device_ids <span class="token operator">=</span> _get_all_device_indices<span class="token punctuation">(</span><span class="token punctuation">)</span>

            self<span class="token punctuation">.</span>device_ids <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> _get_device_index<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device_ids<span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> output_device <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                output_device <span class="token operator">=</span> device_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

            self<span class="token punctuation">.</span>output_device <span class="token operator">=</span> _get_device_index<span class="token punctuation">(</span>output_device<span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> process_group <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>process_group <span class="token operator">=</span> _get_default_group<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>process_group <span class="token operator">=</span> process_group

        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>module <span class="token operator">=</span> module
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device
        self<span class="token punctuation">.</span>broadcast_buffers <span class="token operator">=</span> broadcast_buffers
        self<span class="token punctuation">.</span>find_unused_parameters <span class="token operator">=</span> find_unused_parameters
        self<span class="token punctuation">.</span>require_backward_grad_sync <span class="token operator">=</span> <span class="token boolean">True</span>
        self<span class="token punctuation">.</span>require_forward_param_sync <span class="token operator">=</span> <span class="token boolean">True</span>
        self<span class="token punctuation">.</span>ddp_join_enabled <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>gradient_as_bucket_view <span class="token operator">=</span> gradient_as_bucket_view

        <span class="token keyword">if</span> check_reduction<span class="token punctuation">:</span>
            <span class="token comment"># This argument is no longer used since the reducer</span>
            <span class="token comment"># will ensure reduction completes even if some parameters</span>
            <span class="token comment"># do not receive gradients.</span>
            warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span>
                <span class="token string">"The `check_reduction` argument in `DistributedDataParallel` "</span>
                <span class="token string">"module is deprecated. Please avoid using it."</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">pass</span>

        <span class="token comment"># used for intra-node param sync and inter-node sync as well</span>
        self<span class="token punctuation">.</span>broadcast_bucket_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">250</span> <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">)</span>

        <span class="token comment">#</span>
        <span class="token comment"># reduction bucket size</span>
        self<span class="token punctuation">.</span>bucket_bytes_cap <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>bucket_cap_mb <span class="token operator">*</span> <span class="token number">1024</span> <span class="token operator">*</span> <span class="token number">1024</span><span class="token punctuation">)</span>

        <span class="token comment"># 保证初始状态一样</span>
        <span class="token comment"># Sync params and buffers</span>
        self<span class="token punctuation">.</span>_sync_params_and_buffers<span class="token punctuation">(</span>authoritative_rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># 下拉看源码</span>
        self<span class="token punctuation">.</span>_ddp_init_helper<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_sync_params_and_buffers</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> authoritative_rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        module_states <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>module_states<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_distributed_broadcast_coalesced<span class="token punctuation">(</span>
                module_states<span class="token punctuation">,</span>
                self<span class="token punctuation">.</span>broadcast_bucket_size<span class="token punctuation">,</span>
                authoritative_rank<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_ddp_init_helper</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Initialization helper function that does the following:

        (1) replicating the module from device[0] to the other devices （前文提到 DDP 也支持一个进程多线程利用多卡，类似 DP ，这时候就会用到第一步）
        (2) bucketing the parameters for reductions （把 parameter 分组，梯度通讯时，先得到梯度的会通讯）
        (3) resetting the bucketing states
        (4) registering the grad hooks （创建管理器）
        (5) passing a handle of DDP to SyncBatchNorm Layer （为 SyncBN 准备）
        """</span>

        <span class="token keyword">def</span> <span class="token function">parameters</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">def</span> <span class="token function">model_parameters</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
                ps <span class="token operator">=</span> m<span class="token punctuation">.</span>_former_parameters<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span> \
                    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token string">"_former_parameters"</span><span class="token punctuation">)</span> \
                    <span class="token keyword">else</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span>recurse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
                <span class="token keyword">for</span> p <span class="token keyword">in</span> ps<span class="token punctuation">:</span>
                    <span class="token keyword">yield</span> p

            <span class="token keyword">for</span> m <span class="token keyword">in</span> m<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> recurse <span class="token keyword">else</span> <span class="token punctuation">[</span>m<span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> p <span class="token keyword">in</span> model_parameters<span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">yield</span> p

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>device_ids <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>

            warnings<span class="token punctuation">.</span>warn<span class="token punctuation">(</span>
                <span class="token string">"Single-Process Multi-GPU is not the recommended mode for "</span>
                <span class="token string">"DDP. In this mode, each DDP instance operates on multiple "</span>
                <span class="token string">"devices and creates multiple module replicas within one "</span>
                <span class="token string">"process. The overhead of scatter/gather and GIL contention "</span>
                <span class="token string">"in every forward pass can slow down training. "</span>
                <span class="token string">"Please consider using one DDP instance per device or per "</span>
                <span class="token string">"module replica by explicitly setting device_ids or "</span>
                <span class="token string">"CUDA_VISIBLE_DEVICES. "</span>
            <span class="token punctuation">)</span>

            <span class="token comment"># only create replicas for single-device CUDA modules</span>
            <span class="token comment">#</span>
            <span class="token comment"># TODO: we don't need to replicate params in here. they're always going to</span>
            <span class="token comment"># be broadcasted using larger blocks in broadcast_coalesced, so it might be</span>
            <span class="token comment"># better to not pollute the caches with these small blocks</span>
            self<span class="token punctuation">.</span>_module_copies <span class="token operator">=</span> replicate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">,</span> detach<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>module

            <span class="token keyword">for</span> module_copy <span class="token keyword">in</span> self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> param<span class="token punctuation">,</span> copy_param <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> parameters<span class="token punctuation">(</span>module_copy<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token comment"># Reducer requires param copies have the same strides across replicas.</span>
                    <span class="token comment"># Fixes up copy_param strides in case replicate didn't match param strides.</span>
                    <span class="token keyword">if</span> param<span class="token punctuation">.</span>layout <span class="token keyword">is</span> torch<span class="token punctuation">.</span>strided <span class="token keyword">and</span> param<span class="token punctuation">.</span>stride<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> copy_param<span class="token punctuation">.</span>stride<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                            copy_param<span class="token punctuation">.</span>set_<span class="token punctuation">(</span>copy_param<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
                                                      <span class="token punctuation">.</span>as_strided<span class="token punctuation">(</span>param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>stride<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                                                      <span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>copy_param<span class="token punctuation">)</span><span class="token punctuation">)</span>
                    copy_param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> param<span class="token punctuation">.</span>requires_grad

        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_module_copies <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>module<span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>modules_params <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>parameters<span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>modules_buffers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">list</span><span class="token punctuation">(</span>m<span class="token punctuation">.</span>buffers<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">]</span>

        <span class="token comment"># Build tuple of (module, parameter) for all parameters that require grads.</span>
        modules_and_parameters <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token punctuation">[</span>
                <span class="token punctuation">(</span>module<span class="token punctuation">,</span> parameter<span class="token punctuation">)</span>
                <span class="token keyword">for</span> module <span class="token keyword">in</span> replica<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token keyword">for</span> parameter <span class="token keyword">in</span> <span class="token builtin">filter</span><span class="token punctuation">(</span>
                    <span class="token keyword">lambda</span> parameter<span class="token punctuation">:</span> parameter<span class="token punctuation">.</span>requires_grad<span class="token punctuation">,</span>
                    parameters<span class="token punctuation">(</span>module<span class="token punctuation">,</span> recurse<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token punctuation">]</span> <span class="token keyword">for</span> replica <span class="token keyword">in</span> self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">]</span>

        <span class="token comment"># Build list of parameters.</span>
        parameters <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token builtin">list</span><span class="token punctuation">(</span>parameter <span class="token keyword">for</span> _<span class="token punctuation">,</span> parameter <span class="token keyword">in</span> replica<span class="token punctuation">)</span>
            <span class="token keyword">for</span> replica <span class="token keyword">in</span> modules_and_parameters<span class="token punctuation">]</span>

        <span class="token comment"># Checks if a module will produce a sparse gradient.</span>
        <span class="token keyword">def</span> <span class="token function">produces_sparse_gradient</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">return</span> module<span class="token punctuation">.</span>sparse
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>EmbeddingBag<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">return</span> module<span class="token punctuation">.</span>sparse
            <span class="token keyword">return</span> <span class="token boolean">False</span>

        <span class="token comment"># Build list of booleans indicating whether or not to expect sparse</span>
        <span class="token comment"># gradients for the corresponding parameters.</span>
        expect_sparse_gradient <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token builtin">list</span><span class="token punctuation">(</span>produces_sparse_gradient<span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token keyword">for</span> module<span class="token punctuation">,</span> _ <span class="token keyword">in</span> replica<span class="token punctuation">)</span>
            <span class="token keyword">for</span> replica <span class="token keyword">in</span> modules_and_parameters<span class="token punctuation">]</span>

        <span class="token comment"># The bucket size limit is specified in the constructor.</span>
        <span class="token comment"># Additionally, we allow for a single small bucket for parameters</span>
        <span class="token comment"># that are defined first, such that their gradients don't spill into</span>
        <span class="token comment"># a much larger bucket, adding unnecessary latency after gradient</span>
        <span class="token comment"># computation finishes. Experiments showed 1MB is a reasonable value.</span>
        bucket_indices <span class="token operator">=</span> dist<span class="token punctuation">.</span>_compute_bucket_assignment_by_size<span class="token punctuation">(</span>
            parameters<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">[</span>dist<span class="token punctuation">.</span>_DEFAULT_FIRST_BUCKET_BYTES<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bucket_bytes_cap<span class="token punctuation">]</span><span class="token punctuation">,</span>
            expect_sparse_gradient<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># Note: reverse list of buckets because we want to approximate the</span>
        <span class="token comment"># order in which their gradients are produced, and assume they</span>
        <span class="token comment"># are used in the forward pass in the order they are defined.</span>
        <span class="token comment"># 管理器</span>
        self<span class="token punctuation">.</span>reducer <span class="token operator">=</span> dist<span class="token punctuation">.</span>Reducer<span class="token punctuation">(</span>
            parameters<span class="token punctuation">,</span>
            <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">reversed</span><span class="token punctuation">(</span>bucket_indices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>process_group<span class="token punctuation">,</span>
            expect_sparse_gradient<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>bucket_bytes_cap<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>find_unused_parameters<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>gradient_as_bucket_view<span class="token punctuation">)</span>

        <span class="token comment"># passing a handle to torch.nn.SyncBatchNorm layer</span>
        self<span class="token punctuation">.</span>_passing_sync_batchnorm_handle<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_module_copies<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
<h4 id="完整样例"><a href="#完整样例" class="headerlink" title="完整样例"></a>完整样例</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># -*- coding: utf-8 -*-</span>

<span class="token keyword">import</span> os
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertForSequenceClassification
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed <span class="token keyword">import</span> DistributedSampler

<span class="token keyword">from</span> utils <span class="token keyword">import</span> bert_name<span class="token punctuation">,</span> collate_fn<span class="token punctuation">,</span> load_data_and_labels<span class="token punctuation">,</span> Data
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split
<span class="token keyword">import</span> time

epochs <span class="token operator">=</span> <span class="token number">15</span>
lr <span class="token operator">=</span> <span class="token number">2e-5</span>
batch_size <span class="token operator">=</span> <span class="token number">64</span>


<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token comment"># dist init</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token string">'env://'</span><span class="token punctuation">)</span>
    rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span>
    size <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"cuda:</span><span class="token interpolation"><span class="token punctuation">&#123;</span>local_rank<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

    <span class="token comment"># dataset</span>
    x_text<span class="token punctuation">,</span> y <span class="token operator">=</span> load_data_and_labels<span class="token punctuation">(</span><span class="token string">"./data/rt-polarity.pos"</span><span class="token punctuation">,</span> <span class="token string">"./data/rt-polarity.neg"</span><span class="token punctuation">)</span>
    x_train<span class="token punctuation">,</span> x_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>x_text<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
    train_data <span class="token operator">=</span> Data<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
    test_data <span class="token operator">=</span> Data<span class="token punctuation">(</span>x_test<span class="token punctuation">,</span> y_test<span class="token punctuation">)</span>
    train_sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>train_data<span class="token punctuation">)</span>
    train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token operator">*</span>size<span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>collate_fn<span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>
    test_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token operator">*</span>size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                             collate_fn<span class="token operator">=</span>collate_fn<span class="token punctuation">,</span> sampler<span class="token operator">=</span>DistributedSampler<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># model</span>
    model <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>bert_name<span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token operator">*</span>size<span class="token punctuation">)</span>
    best_acc <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">0.1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"rank </span><span class="token interpolation"><span class="token punctuation">&#123;</span>rank<span class="token punctuation">&#125;</span></span><span class="token string"> start training..."</span></span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        total_loss <span class="token operator">=</span> <span class="token number">0.0</span>
        train_sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> step<span class="token punctuation">,</span> batch_data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> batch_data
            inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">,</span> labels<span class="token operator">=</span>labels<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            total_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        acc <span class="token operator">=</span> test<span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_loader<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
        <span class="token keyword">if</span> acc <span class="token operator">></span> best_acc<span class="token punctuation">:</span>
            best_acc <span class="token operator">=</span> acc
            <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\t Epoch</span><span class="token interpolation"><span class="token punctuation">&#123;</span>epoch<span class="token punctuation">&#125;</span></span><span class="token string">: loss: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>total_loss<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">, acc: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>acc<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">, time: </span><span class="token interpolation"><span class="token punctuation">&#123;</span><span class="token punctuation">(</span>end_time <span class="token operator">-</span> start_time<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">&#125;</span></span><span class="token string">s"</span></span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"*"</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"finished; best acc: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>best_acc<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> test_loader<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    preds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> data <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
            inputs<span class="token punctuation">,</span> truth <span class="token operator">=</span> data
            inputs <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            truth <span class="token operator">=</span> truth<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'logits'</span><span class="token punctuation">]</span>
            predict <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

            cur_preds <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>predict<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
            cur_truth <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>truth<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>cur_preds<span class="token punctuation">,</span> predict<span class="token punctuation">)</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>cur_truth<span class="token punctuation">,</span> truth<span class="token punctuation">)</span>

            preds<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>cur_preds<span class="token punctuation">)</span>
            labels<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>cur_truth<span class="token punctuation">)</span>

    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    predict <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    correct <span class="token operator">=</span> <span class="token punctuation">(</span>predict <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> correct <span class="token operator">*</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>predict<span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
	train<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h2><blockquote>
<p>DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.</p>
</blockquote>
<h3 id="Innovation-pillars"><a href="#Innovation-pillars" class="headerlink" title="Innovation pillars"></a>Innovation pillars</h3><p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/3pillars.png" alt="img"></p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><h4 id="Argument-Parsing"><a href="#Argument-Parsing" class="headerlink" title="Argument Parsing"></a>Argument Parsing</h4><ul>
<li>将<code>argparse</code>变成<code>deepspeed parser</code></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">parser <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>add_config_arguments<span class="token punctuation">(</span>parser<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<ul>
<li>样例</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> argparse
<span class="token keyword">import</span> deepspeed

<span class="token keyword">def</span> <span class="token function">add_argument</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    parser<span class="token operator">=</span>argparse<span class="token punctuation">.</span>ArgumentParser<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">'CIFAR'</span><span class="token punctuation">)</span>

    <span class="token comment">#data</span>
    <span class="token comment"># cuda</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--with_cuda'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'use CPU in case there\'s no GPU support'</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--use_ema'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'whether use exponential moving average'</span><span class="token punctuation">)</span>

    <span class="token comment"># train</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-b'</span><span class="token punctuation">,</span> <span class="token string">'--batch_size'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'mini-batch size (default: 32)'</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'-e'</span><span class="token punctuation">,</span> <span class="token string">'--epochs'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span>
                        <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'number of total epochs (default: 30)'</span><span class="token punctuation">)</span>
    parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--local_rank'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>
                       <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'local rank passed from distributed launcher'</span><span class="token punctuation">)</span>

    <span class="token comment"># Include DeepSpeed configuration arguments</span>
    parser <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>add_config_arguments<span class="token punctuation">(</span>parser<span class="token punctuation">)</span>

    args<span class="token operator">=</span>parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> args<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h4><ul>
<li>必须把<code>args, model结构和参数</code>变成deepspeed版本</li>
<li>对于<code>dataloder</code>可以用<code>deepspeed.initialize</code>变成分布式（需要传入<code>trainset</code>），也可以自定义(不需要传入<code>trainset</code>)</li>
<li>API</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">initialize</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span>
               model<span class="token punctuation">,</span>
               optimizer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               model_parameters<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               training_data<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               lr_scheduler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               mpu<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
               dist_init_required<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
               collate_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>样例</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">parameters <span class="token operator">=</span> <span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">,</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
args<span class="token operator">=</span>add_argument<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Initialize DeepSpeed to use the following features</span>
<span class="token comment"># 1) Distributed model</span>
<span class="token comment"># 2) Distributed data loader</span>
<span class="token comment"># 3) DeepSpeed optimizer</span>

<span class="token comment"># 需要传入`trainset`</span>
model_engine<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> trainloader<span class="token punctuation">,</span> _ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>args<span class="token operator">=</span>args<span class="token punctuation">,</span> model<span class="token operator">=</span>net<span class="token punctuation">,</span> model_parameters<span class="token operator">=</span>parameters<span class="token punctuation">,</span> training_data<span class="token operator">=</span>trainset<span class="token punctuation">)</span>
<span class="token comment"># 不需要传入`trainset`</span>
model_engine<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>args<span class="token operator">=</span>args<span class="token punctuation">,</span> model<span class="token operator">=</span>net<span class="token punctuation">,</span> model_parameters<span class="token operator">=</span>parameters<span class="token punctuation">,</span> training_data<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="DataLoder"><a href="#DataLoder" class="headerlink" title="DataLoder"></a>DataLoder</h4><ul>
<li>因为deepspeed基于pytorch.dist，所以可以用dataloader，注意自定义dataloder需要分布式采样<code>DistributedSampler</code></li>
<li>自定义分布式dataloder，注意batch_size定义，分布式的batch_size是指每张gpu上批大小，不开启分布式batch_size指所有gpu的总大小</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 开启分布式 用DistributedSampler</span>
<span class="token keyword">if</span> local_rank <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span> 
    <span class="token keyword">if</span> data_sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        data_sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
        device_count <span class="token operator">=</span> <span class="token number">1</span>

<span class="token comment"># 不开启分布式 用 RandomSampler</span>
<span class="token keyword">else</span><span class="token punctuation">:</span> 
    <span class="token keyword">if</span> data_sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        data_sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
        device_count <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span>
        batch_size <span class="token operator">*=</span> device_count
            
self<span class="token punctuation">.</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
    pin_memory<span class="token operator">=</span>self<span class="token punctuation">.</span>pin_memory<span class="token punctuation">,</span>
    sampler<span class="token operator">=</span>self<span class="token punctuation">.</span>data_sampler<span class="token punctuation">,</span>
    num_workers<span class="token operator">=</span>self<span class="token punctuation">.</span>num_local_io_workers<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>最好定义成生成器（相当于<code>IterableDataset</code>）</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>_create_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> self

<span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span><span class="token builtin">len</span>

<span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>tput_timer<span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>tput_timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token builtin">next</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>样例</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> logging
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> RandomSampler
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed <span class="token keyword">import</span> DistributedSampler
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm


<span class="token keyword">class</span> <span class="token class-name">DeepSpeedDataLoader</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 dataset<span class="token punctuation">,</span>
                 batch_size<span class="token punctuation">,</span>
                 pin_memory<span class="token punctuation">,</span>
                 local_rank<span class="token punctuation">,</span>
                 tput_timer<span class="token punctuation">,</span>
                 collate_fn<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 num_local_io_workers<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 data_sampler<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>tput_timer <span class="token operator">=</span> tput_timer
        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size

        <span class="token keyword">if</span> local_rank <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">:</span> <span class="token comment"># 开启分布式 用DistributedSampler</span>
            <span class="token keyword">if</span> data_sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                data_sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
            device_count <span class="token operator">=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span> <span class="token comment"># 不开启分布式 用 RandomSampler</span>
            <span class="token keyword">if</span> data_sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                data_sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">)</span>
            device_count <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span>
            batch_size <span class="token operator">*=</span> device_count

        <span class="token keyword">if</span> num_local_io_workers <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            num_local_io_workers <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> device_count

        self<span class="token punctuation">.</span>num_local_io_workers <span class="token operator">=</span> num_local_io_workers
        self<span class="token punctuation">.</span>data_sampler <span class="token operator">=</span> data_sampler
        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset
        self<span class="token punctuation">.</span>collate_fn <span class="token operator">=</span> collate_fn
        self<span class="token punctuation">.</span>device_count <span class="token operator">=</span> device_count
        self<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> batch_size
        self<span class="token punctuation">.</span>pin_memory <span class="token operator">=</span> pin_memory
        self<span class="token punctuation">.</span><span class="token builtin">len</span> <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_sampler<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>_create_dataloader<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span><span class="token builtin">len</span>

    <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>tput_timer<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>tput_timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">next</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_create_dataloader</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>collate_fn <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">,</span>
                                         batch_size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
                                         pin_memory<span class="token operator">=</span>self<span class="token punctuation">.</span>pin_memory<span class="token punctuation">,</span>
                                         sampler<span class="token operator">=</span>self<span class="token punctuation">.</span>data_sampler<span class="token punctuation">,</span>
                                         num_workers<span class="token operator">=</span>self<span class="token punctuation">.</span>num_local_io_workers<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">,</span>
                                         batch_size<span class="token operator">=</span>self<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>
                                         pin_memory<span class="token operator">=</span>self<span class="token punctuation">.</span>pin_memory<span class="token punctuation">,</span>
                                         sampler<span class="token operator">=</span>self<span class="token punctuation">.</span>data_sampler<span class="token punctuation">,</span>
                                         collate_fn<span class="token operator">=</span>self<span class="token punctuation">.</span>collate_fn<span class="token punctuation">,</span>
                                         num_workers<span class="token operator">=</span>self<span class="token punctuation">.</span>num_local_io_workers<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>dataloader<span class="token punctuation">)</span>

        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dataloader<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Training-API"><a href="#Training-API" class="headerlink" title="Training API"></a>Training API</h4><ul>
<li>把读取的分布式<code>data</code>放到相应的显卡上 <code>inputs, labels = data[0].to(model_engine.local_rank), data[1].to(model_engine.local_rank)</code></li>
<li>注意<code>loss</code>和<code>optimizer</code>分别是<code>model_engine.backward(loss)</code>和<code>model_engine.step()</code>,不需要<code>optimizer.zero_grad()</code>. (Zeroing the gradients is handled automatically by DeepSpeed after the weights have been updated using a mini-batch.)</li>
<li>样例</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> data <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>trainloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># get the inputs; data is a list of [inputs, labels]</span>
    inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model_engine<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model_engine<span class="token punctuation">.</span>local_rank<span class="token punctuation">)</span>

    outputs <span class="token operator">=</span> model_engine<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

    model_engine<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    model_engine<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><ul>
<li>定义 <code>JSON file (ds_config.json)</code></li>
</ul>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>
  <span class="token property">"train_batch_size"</span><span class="token operator">:</span> <span class="token number">4</span><span class="token punctuation">,</span>
  <span class="token property">"steps_per_print"</span><span class="token operator">:</span> <span class="token number">2000</span><span class="token punctuation">,</span>
  <span class="token property">"optimizer"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"Adam"</span><span class="token punctuation">,</span>
    <span class="token property">"params"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      <span class="token property">"lr"</span><span class="token operator">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span>
      <span class="token property">"betas"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token number">0.8</span><span class="token punctuation">,</span>
        <span class="token number">0.999</span>
      <span class="token punctuation">]</span><span class="token punctuation">,</span>
      <span class="token property">"eps"</span><span class="token operator">:</span> <span class="token number">1e-8</span><span class="token punctuation">,</span>
      <span class="token property">"weight_decay"</span><span class="token operator">:</span> <span class="token number">3e-7</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
  <span class="token property">"scheduler"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
    <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"WarmupLR"</span><span class="token punctuation">,</span>
    <span class="token property">"params"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      <span class="token property">"warmup_min_lr"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
      <span class="token property">"warmup_max_lr"</span><span class="token operator">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span>
      <span class="token property">"warmup_num_steps"</span><span class="token operator">:</span> <span class="token number">1000</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
  <span class="token property">"wall_clock_breakdown"</span><span class="token operator">:</span> <span class="token boolean">false</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="Running"><a href="#Running" class="headerlink" title="Running"></a>Running</h4><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ deepspeed deepspeed.py <span class="token parameter variable">--deepspeed</span> <span class="token parameter variable">--deepspeed_config</span> ds_config.json<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="Inference-API"><a href="#Inference-API" class="headerlink" title="Inference API"></a>Inference API</h4><ul>
<li><a target="_blank" rel="noopener" href="https://deepspeed.readthedocs.io/en/latest/inference-init.html#deepspeed.init_inference"><code>deepspeed.init_inference()</code></a> returns an <em>inference engine</em> of type <code>InferenceEngine</code>.</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> step<span class="token punctuation">,</span> batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment">#forward() method</span>
    loss <span class="token operator">=</span> engine<span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>



<h2 id="Accelerator"><a href="#Accelerator" class="headerlink" title="Accelerator"></a>Accelerator</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/huggingface/accelerate">https://github.com/huggingface/accelerate</a></p>
</li>
<li><p>使用</p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> accelerate
accelerator <span class="token operator">=</span> accelerate<span class="token punctuation">.</span>Accelerator<span class="token punctuation">(</span><span class="token punctuation">)</span>
device <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>device <span class="token comment">#获取当前进程的设备</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token comment"># 进行封装</span>
model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> dataloader <span class="token operator">=</span> accelerator<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> dataloader<span class="token punctuation">)</span>

<span class="token comment">#训练时 loss.backward() 换为：</span>
accelerator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p><code>accelerator.print</code>：仅仅在主进程输出</p>
</li>
<li><p><code>accelerator.process_index</code>: 当前进程ID，没有使用rank命名，而是用的process_index来表示</p>
</li>
<li><p><code>accelerator.is_local_main_process/is_main_processs:</code>: 是否local_rank 或则rank为0， 主进程</p>
</li>
<li><p><code>accelerator.wait_for_everyone()</code> ： 类似 dist.barrier() , 等所有进程到达这一步。</p>
</li>
<li><p><code>accelerator.save</code>： 保存模型</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/image-20230817153232090.png" alt="image-20230817153232090"></p>
<h2 id="Horovod"><a href="#Horovod" class="headerlink" title="Horovod"></a>Horovod</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/horovod/horovod">https://github.com/horovod/horovod</a></p>
</li>
<li><p>使用</p>
</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> horovod<span class="token punctuation">.</span>torch <span class="token keyword">as</span> hvd
<span class="token comment"># 初始化</span>
hvd<span class="token punctuation">.</span>init<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># Samapler</span>
<span class="token comment"># *此处num_replicas=hvd.size(), rank=hvd.rank()必须*</span>
train_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>
    train_dataset<span class="token punctuation">,</span> num_replicas<span class="token operator">=</span>hvd<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> rank<span class="token operator">=</span>hvd<span class="token punctuation">.</span>rank<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">)</span>
<span class="token comment"># 优化器包装</span>
optimizer <span class="token operator">=</span> hvd<span class="token punctuation">.</span>DistributedOptimizer<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> named_parameters<span class="token operator">=</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 模型分发广播</span>
hvd<span class="token punctuation">.</span>broadcast_parameters<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> root_rank<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># 模型训练不需要修改</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python">$ horovodrun <span class="token operator">-</span>np <span class="token number">4</span> <span class="token operator">-</span>H localhost<span class="token punctuation">:</span><span class="token number">4</span> python3 train<span class="token punctuation">.</span>py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>



<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a target="_blank" rel="noopener" href="https://shomy.top/2022/01/05/torch-ddp-intro/#horovod">https://shomy.top/2022/01/05/torch-ddp-intro/#horovod</a></li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Lee Jet</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://lee-jet.github.io/2023/08/17/distributed-training/">https://lee-jet.github.io/2023/08/17/distributed-training/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Lee Jet</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Paper-Reading/">
                                    <span class="chip bg-color">Paper Reading</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '239ba579785b9bb8ffde',
        clientSecret: '6b8159892d8d5785b7be9a84c0998462ce8649d4',
        repo: 'gitalk',
        owner: 'lee-jet',
        admin: "lee-jet",
        id: '2023-08-17T15-58-48',
        distractionFreeMode: false,  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/08/21/chatgpt/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="ChatGPT">
                        
                        <span class="card-title">ChatGPT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-08-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Paper-Reading/" class="post-category">
                                    Paper Reading
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Paper-Reading/">
                        <span class="chip bg-color">Paper Reading</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/08/16/10-shell/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/18.jpg" class="responsive-img" alt="10-shell">
                        
                        <span class="card-title">10-shell</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-08-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Tech-share/" class="post-category">
                                    Tech share
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Tech-share/">
                        <span class="chip bg-color">Tech share</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2024</span>
            
            <a href="/about" target="_blank">Lee Jet</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">143.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/lee-jet" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1874087716@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1874087716" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1874087716" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
