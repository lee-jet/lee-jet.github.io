<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="model-compression-and-acceleration.md, AiHub">
    <meta name="description" content="
      An Overview of Model Compression and Acceleration    
    
     Author:Jet Date:2023/07 


Background
减少模型存储和计算成本">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>model-compression-and-acceleration.md | AiHub</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>
	
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4517244572130756"
		 crossorigin="anonymous"></script>
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="AiHub" type="application/atom+xml">
</head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">AiHub</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">AiHub</div>
        <div class="logo-desc">
            
            Fire
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/lee-jet" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/lee-jet" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/4.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">model-compression-and-acceleration.md</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Tech-share/">
                                <span class="chip bg-color">Tech share</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-07-27
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2023-07-31
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <div align='center' >
    <font size='5'> <b> An Overview of Model Compression and Acceleration  </b> </font> 
    <br>
    <font size='4'> Author:Jet Date:2023/07 </font>
</div>

<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol>
<li><p>减少模型存储和计算成本</p>
</li>
<li><p>期望模型不仅能部署在服务端GPU，也能部署在移动端</p>
</li>
<li><p>神经网络中卷积层、全连接层权重参数具有冗余的特点</p>
<p> 卷积层占据了大约 90-95% 的计算时间和参数规模，有较大的值；全连接层占据了大约 5-10% 的计算时间，95% 的参数规模，并且值较小</p>
</li>
</ol>
<h2 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h2><p>​    综合现有的深度模型压缩方法，它们主要分为四类: </p>
<ul>
<li>参数剪枝和共享（parameter pruning and sharing）<ul>
<li>针对模型参数的冗余性，试图去除冗余和不重要的项</li>
</ul>
</li>
<li>低秩因子分解（low-rank factorization）<ul>
<li>使用矩阵/张量分解来估计深度学习模型的信息参数</li>
</ul>
</li>
<li>转移/紧凑卷积滤波器（transferred/compact convolutional filters）<ul>
<li>特殊的结构卷积滤波器来降低存储和计算复杂度</li>
</ul>
</li>
<li>知识蒸馏（knowledge distillation）<ul>
<li>训练一个更紧凑的神经网络来重现一个更大的网络的输出</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307271509286.png" alt="image-20230727150746346"></p>
<p>​        一般来说，参数修剪和共享，低秩分解和知识蒸馏方法可以用于全连接层和卷积层的 CNN，但另一方面，使用转移/紧凑型卷积核的方法仅支持卷积层。低秩因子分解和基于转换/紧凑型卷积核的方法提供了一个端到端的流水线，可以很容易地在 CPU/GPU 环境中实现。相反参数修剪和共享使用不同的方法，如矢量量化，二进制编码和稀疏约束来执行任务，这导致常需要几个步骤才能达到目标。</p>
<p>​        基于参数修剪/共享、低秩分解的模型可以从预训练模型或者从头开始训练，因此灵活而有效。然而转移/紧凑的卷积核和知识蒸馏模型只能支持从零开始训练。</p>
<h3 id="参数修剪和共享"><a href="#参数修剪和共享" class="headerlink" title="参数修剪和共享"></a>参数修剪和共享</h3><p>​    三类：模型量化和二进制化、参数共享和结构化矩阵（structural matrix）</p>
<ul>
<li>网络量化(quantization)通过减少表示每个权重所需的比特数来压缩原始网络：fp32 fp16 int8 int4</li>
<li>修剪减少了需要编码的权重数量，量化和霍夫曼编码减少了用于对每个权重编码的比特数。对于大部分元素为 0 的矩阵可以使用稀疏表示，进一步降低空间冗余，且这种压缩机制不会带来任何准确率损失。 </li>
<li>剪枝:</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307271509284.png" alt="image-20230727145531991"></p>
<h3 id="低秩分解和稀疏性"><a href="#低秩分解和稀疏性" class="headerlink" title="低秩分解和稀疏性"></a>低秩分解和稀疏性</h3><p>​        典型的 CNN 卷积核是一个 4D 张量，而全连接层也可以当成一个 2D 矩阵，低秩分解同样可行。这些张量中可能存在大量的冗余。所有近似过程都是逐层进行的，在一个层经过低秩滤波器近似之后，该层的参数就被固定了，而之前的层已经用一种重构误差标准（reconstruction error criterion）微调过。</p>
<h3 id="迁移-压缩卷积滤波器"><a href="#迁移-压缩卷积滤波器" class="headerlink" title="迁移/压缩卷积滤波器"></a>迁移/压缩卷积滤波器</h3><p>​        在 Inception 结构中使用了将 3×3 卷积分解成两个 1×1 的卷积；SqueezeNet 提出用 1×1 卷积来代替 3×3 卷积，与 AlexNet 相比，SqueezeNet 创建了一个紧凑的神经网络，参数少了 50 倍，准确度相当。</p>
<h3 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h3><p>​    “学生-教师”的范式，即通过软化“教师”的输出而惩罚“学生”</p>
<p>​    只能用于具有 Softmax 损失函数分类任务</p>
<blockquote>
<p>TensorFlow 支持的是一种静态图，当模型的参数确定之后，便无法继续修改。这对于逐阶段、分层的训练带来了一定的困难。相比之下，Pytorch 使用了动态图，在定义完模型之后还可以边训练边修改其参数，具有很高的灵活性。这也是深度学习未来的发展方向</p>
</blockquote>
<h2 id="YOLOv6"><a href="#YOLOv6" class="headerlink" title="YOLOv6"></a>YOLOv6</h2><p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149762.png" alt="image-20230731114838134"></p>
<h3 id="EfficientRep-Backbone"><a href="#EfficientRep-Backbone" class="headerlink" title="EfficientRep Backbone"></a>EfficientRep Backbone</h3><ul>
<li>多分支的网络(ResNet,DenseNet,GoogLeNet)相比单分支(ImageNet,VGG)的通常能够有更好的分类性能。但是，它通常伴随着并行性的降低，并导致推理延迟的增加。相反，像VGG这样的普通单路径网络具有高并行性和较少内存占用的优点，从而导致更高的推理效率。</li>
<li>RepVGG Style 结构是一种在训练时具有多分支拓扑，而在实际部署时可以等效融合为单个 3x3 卷积的一种可重参数化的结构。通过融合成的 3x3 卷积结构，可以有效利用计算密集型硬件计算能力（比如 GPU），同时也可获得 GPU/CPU 上已经高度优化的 NVIDIA cuDNN 和 Intel MKL 编译框架的帮助。</li>
</ul>
<table>
<thead>
<tr>
<th><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149764.png" alt="image-20230731101347070"></th>
<th><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149765.png" alt="image-20230731101714496"></th>
</tr>
</thead>
</table>
<h3 id="Rep-PAN-neck"><a href="#Rep-PAN-neck" class="headerlink" title="Rep-PAN neck"></a>Rep-PAN neck</h3><ul>
<li>将YOLOv5中使用的 <strong>CSPBlock替换为RepBlock（适用于小型模型）或CSPStackRep Block（用于大型模型）</strong>，并相应调整宽度和深度。</li>
</ul>
<h3 id="Decoupled-Head"><a href="#Decoupled-Head" class="headerlink" title="Decoupled Head"></a>Decoupled Head</h3><ul>
<li><p>原始 YOLOv5 的检测头是通过分类和回归分支融合共享的方式来实现的，而 YOLOX 的检测头则是将分类和回归分支进行解耦，同时新增了两个额外的 3x3 的卷积层，虽然提升了检测精度，但一定程度上增加了网络延时。</p>
</li>
<li><p>因此，我们对解耦头进行了精简设计，同时综合考虑到相关算子表征能力和硬件上计算开销这两者的平衡，采用 <strong>Hybrid Channels</strong> 策略重新设计了一个更高效的解耦头结构，在维持精度的同时降低了延时，缓解了解耦头中 3x3 卷积带来的额外延时开销。通过在 nano 尺寸模型上进行消融实验，对比相同通道数的解耦头结构，精度提升 0.2% AP 的同时，速度提升6.8%。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149766.png" alt="image-20230731103229622"></p>
<h3 id="Training-strategies"><a href="#Training-strategies" class="headerlink" title="Training strategies"></a>Training strategies</h3><h4 id="Anchor-free"><a href="#Anchor-free" class="headerlink" title="Anchor-free"></a>Anchor-free</h4><ul>
<li>由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度；同时，在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时。而 Anchor-free 无锚范式因其泛化能力强，解码逻辑更简单，在近几年中应用比较广泛。经过对 Anchor-free 的实验调研，我们发现，相较于Anchor-based 检测器的复杂度而带来的额外延时，Anchor-free 检测器在速度上有51%的提升。有两种类型的anchor-free检测器：point-based(YOLOX,FCOS)和keypoing-based(CenterNet)。在YOLOv6中，采用了 anchor-free point-based范式。</li>
</ul>
<h4 id="TAL"><a href="#TAL" class="headerlink" title="TAL"></a>TAL</h4><ul>
<li><p>OTA将目标检测中的标签分配视为最佳传输的问题。它从全局角度定义了每个GT目标的正/负训练样本。SimOTA是OTA的简化版本，它减少了额外的超参数并保持了性能。在YOLOv6的早期版本中，SimOTA被用作标签分配方法。然而，在实践中，发现引入SimOTA会减缓训练过程。而且，陷入不稳定训练的情况经常出现。因此，设计了一个 SimOTA的替代品TAL。</p>
</li>
<li><p>Task alignment learning：任务对齐学习（TAL），TOOD: Task-aligned One-stage Object Detection任务对齐的单阶段目标检测。由于分类和定位的学习机制不同，两个任务学习到的特征的空间分布可能不同，当使用两个单独的分支进行预测时，会导致一定程度的错位。增加两个任务之间的交互，(2)增强检测器学习比对的能力</p>
</li>
</ul>
<h4 id="Activation"><a href="#Activation" class="headerlink" title="Activation"></a>Activation</h4><ul>
<li><p>YOLO系列激活函数各不相同，如ReLU、LReLU、Swish、SiLU、Mish等。在这些激活函数中，SiLU是使用最多的。<strong>一般来说，SiLU的精度更高，不会造成太多额外的计算成本</strong>。然而，在工业应用中，特别是在部署具有<strong>TensorRT加速的模型时，ReLU具有更大的速度优势，因为它融合到了卷积中。</strong></p>
</li>
<li><p>此外，进一步验证了RepConv/普通卷积（表示为Conv）和ReLU/SiLU/LReLU组合在不同大小网络中的有效性，以实现更好的折衷。带有SiLU的Conv在精度上表现最佳， 而RepConv和ReLU的组合实现了更好的权衡。所以建议用户在对延迟敏感的应用程序中使用RepConv和ReLU。</p>
</li>
<li><p>使用RepConv/ReLU在YOLOv6-N/T/S/M中，用于更高的推理速度；在大型型号 YOLOv6-L中使用Conv/SiLU组合加速训练并提高性能</p>
</li>
</ul>
<h4 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h4><ul>
<li><p>Classification Loss</p>
<ul>
<li>Focal Loss修改了传统的交叉熵损失，以解决正负样本或难易样本之间的类不平衡问题。为了解决训练和推理之间质量估计和分类的不一致使用，Quality Focal Loss（QFL）进一步扩展了Focal Loss，将分类分数和本地化质量联合表示用于分类监督。而VariFocal Loss（VFL）源于Focal Loss，但它不对称地处理正样本和负样本。通过考虑不同重要程度的正样本和负样本，它平衡了来自两个样本的学习信号。Poly Loss将常用的分类损失分解为一系列加权多项式基。它在不同的任务和数据集上调整多项式系数，通过实验证明其优于交叉熵损失和Focal Loss<ul>
<li>在YOLOv6-N/S/M上实验了Focal Los、Polyloss、QFL和VFL。如表8所示，与Focal Loss相比，VFL对YOLOv6-N/S/M分别带来0.2%/0.3%/0.1%的AP改善。所以，选择 <strong>VFL作为分类损失函数</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>Box Regression Loss</p>
<p>  ​        框回归损失为精确定位边界盒提供了重要的学习信号。L1 Loss是早期工作中的原始框回归损失。逐渐地，各种精心设计的框回归损失如IoU系列Loss和Probability Loss如雨后春笋般涌现。</p>
<ul>
<li><p>IoU-series Loss： IoU Loss将预测框的四个边界作为一个整体单位进行回归。由于其与评价指标的一致性，已被证明是有效的。IoU有许多变体，如GIoU、DIoU、CIoU、α-IoU和SIoU等，形成相关损失函数。对GIoU、CIoU和SIoU进行了实验。 SIoU应用于YOLOv6-N和YOLOV 6-T，而其他的使用GIoU</p>
<ul>
<li><strong>对于 YOLOv6-N和YOLOv 6-T，SIoU Loss优于其他Loss，而对于 YOLOv6-M，CIoU Loss表现更好</strong></li>
</ul>
</li>
<li><p>Probability Loss  </p>
<ul>
<li>Dostronition Focal Loss（DFL）将框位置的基本连续分布简化为离散化概率分布。**它考虑了数据中的模糊性和不确定性，而不引入任何其他先验，这有助于提高框定位精度，特别是当GT框的边界模糊时。基于DFL，DFLv2开发了一个轻量级子网络，以利用分布统计和实际定位质量之间的密切相关性，进一步提高检测性能。然而，DFL通常输出比一般盒回归多17倍的回归值，导致大量开销。额外的计算成本显著阻碍了小模型的训练。</li>
<li><strong>仅在YOLOv6-M/L中采用DFL</strong></li>
</ul>
</li>
<li><p>Object Loss</p>
<ul>
<li>FCOS中首次提出了Object Loss，以降低低质量边界框的分数，以便在后处理中过滤掉它们。在YOLOX中也使用了它来加速收敛并提高网络精度。作为一个像FCOS和YOLOX这样的anchor-free框架，本文尝试将Object Loss引入YOLOv6。不幸的是，它没有带来很多积极的效果。</li>
<li>在 <strong>YOLOv6中去除Object Loss</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Epochs"><a href="#Epochs" class="headerlink" title="Epochs"></a>Epochs</h4><ul>
<li> <strong>随着训练时间的增加，检测器的性能不断提高</strong>。我们将训练时间从300个周期延长到400个周期，以达到更好的收敛。</li>
</ul>
<h4 id="Self-distillation"><a href="#Self-distillation" class="headerlink" title="Self-distillation"></a>Self-distillation</h4><ul>
<li>在评估YOLOv5和YOLOv7中的模型性能时，在每个图像周围放置了half-stride灰色边界（就是resize图片时四周填充了灰度的padding）。虽然没有添加有用的信息，但它有助于检测图像边缘附近的对象。这个技巧也适用于YOLOv6</li>
<li>然而，额外的灰度像素明显降低了推理速度。如果没有灰色边界，YOLOv6的性能会恶化，这也是YOLOv5和YOLOv7中遇到的情况。我们假设该问题与Mosaic增强中的灰色边界填充有关。<strong>为了验证，进行了在最后一个epoch时关闭Mosaic增强的实验（又名衰退策略,fade strategy）。</strong>在这方面，我们改变了灰度边界的面积，并将具有灰度边界的图像直接调整为目标图像大小。结合这两种策略，我们的模型可以在不降低推理速度的情况下保持甚至提高性能</li>
<li>一方面，性能下降的问题得到了缓解。另一方面，无论是否填充灰色边界，小模型（YOLOv6-N/S）的精度都会提高</li>
<li> 将输入图像限制为634×634，并在边缘周围添加3像素宽的灰色边界。使用该策略，最终图像的大小为预期的640×640。当最终图像尺寸从672减小到640时，YOLOv6-N/S/M的最终性能甚至更精确0.2%/0.3%/0.1%</li>
</ul>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md">https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md</a><ul>
<li>step 1 Training with RepOptimizer</li>
<li>step 2 PTQ + sensitivity analyze</li>
<li>step 3 QAT: channel-wise distillation</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307271718700.png" alt="image-20230727171635419"></p>
<h3 id="RepOptimizers"><a href="#RepOptimizers" class="headerlink" title="RepOptimizers"></a>RepOptimizers</h3><p>​    重参数化优化器, 针对VGG这种结构进行优化</p>
<p>​    将先验信息用于修改梯度数值，称为梯度重参数化，对应的优化器称为RepOptimizer。我们着重关注VGG式的直筒模型，训练得到RepOptVGG模型，他有着高训练效率，简单直接的结构和极快的推理速度。</p>
<ul>
<li><p>paper <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.15242.pdf">https://arxiv.org/pdf/2205.15242.pdf</a> </p>
</li>
<li><p>code <a target="_blank" rel="noopener" href="https://github.com/DingXiaoH/RepOptimizers">https://github.com/DingXiaoH/RepOptimizers</a></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/531477704">https://zhuanlan.zhihu.com/p/531477704</a></li>
</ul>
</li>
<li><p><strong>与RepVGG的区别</strong></p>
</li>
</ul>
<ol>
<li>RepVGG加入了结构先验（如1x1，identity分支），并使用常规优化器训练。而RepOptVGG则是<strong>将这种先验知识加入到优化器实现中</strong></li>
<li>尽管RepVGG在推理阶段可以把各分支融合，成为一个直筒模型。但是<strong>其训练过程中有着多条分支，需要更多显存和训练时间</strong>。而RepOptVGG可是 <strong>真-直筒模型</strong>，从训练过程中就是一个VGG结构</li>
<li>我们通过定制优化器，实现了结构重参数化和梯度重参数化的等价变换，这种变换是通用的，可以拓展到更多模型</li>
</ol>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md">https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md</a></p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">quant_sensitivity_analyse</span><span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> evaler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># disable all quantable layer</span>
    model_quant_disable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">)</span>
    
    <span class="token comment"># analyse each quantable layer</span>
    quant_sensitivity <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> k<span class="token punctuation">,</span> m <span class="token keyword">in</span> model_ptq<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>QuantConv2d<span class="token punctuation">)</span> <span class="token keyword">or</span> \
           <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>QuantConvTranspose2d<span class="token punctuation">)</span> <span class="token keyword">or</span> \
           <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            module_quant_enable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># module can not be quantized, continue</span>
            <span class="token keyword">continue</span>

        eval_result <span class="token operator">=</span> evaler<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span>model_ptq<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>eval_result<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Quantize Layer &#123;&#125;, result mAP0.5 = &#123;:0.4f&#125;, mAP0.5:0.95 = &#123;:0.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span>
                                                                                          eval_result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                                          eval_result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        quant_sensitivity<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> eval_result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> eval_result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># disable this module sensitivity, anlayse next module</span>
        module_quant_disable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">)</span>

    <span class="token keyword">return</span> quant_sensitivity<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>EfficientRep：An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design</p>
<ul>
<li> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.00386">https://arxiv.org/abs/2302.00386</a></li>
</ul>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>
    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'EfficientRep'</span><span class="token punctuation">,</span>
    num_repeats<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    out_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>
    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RepPANNeck'</span><span class="token punctuation">,</span>
    num_repeats<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    out_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">,</span>
head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>
    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'EffiDeHead'</span><span class="token punctuation">,</span>
    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    num_layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    begin_indices<span class="token operator">=</span><span class="token number">24</span><span class="token punctuation">,</span>
    anchors<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    out_indices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    atss_warmup_epoch<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
    iou_type<span class="token operator">=</span><span class="token string">'giou'</span><span class="token punctuation">,</span>
    use_dfl<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    reg_max<span class="token operator">=</span><span class="token number">0</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><p>用了RepVGGOptimizer SGD针对VGGBlock进行优化</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/yolov6/utils/RepOptimizer.py">https://github.com/meituan/YOLOv6/blob/main/yolov6/utils/RepOptimizer.py</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/DingXiaoH/RepOptimizers/blob/main/repoptimizer/repoptvgg_impl.py">https://github.com/DingXiaoH/RepOptimizers/blob/main/repoptimizer/repoptvgg_impl.py</a></li>
<li>不同的优化器SGD AdamW配置不同 <ul>
<li>In the <code>step</code> function, RepOptimizers will use the Grad Mults properly. For SGD, please see <a target="_blank" rel="noopener" href="https://github.com/DingXiaoH/RepOptimizers/blob/main/repoptimizer/repoptimizer_sgd.py#L38">here</a>. For AdamW, please see <a target="_blank" rel="noopener" href="https://github.com/DingXiaoH/RepOptimizers/blob/main/repoptimizer/repoptimizer_adamw.py#L145">here</a> and <a target="_blank" rel="noopener" href="https://github.com/DingXiaoH/RepOptimizers/blob/main/repoptimizer/repoptimizer_adamw.py#L274">here</a>.</li>
</ul>
</li>
</ul>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> <span class="token punctuation">.</span><span class="token punctuation">.</span>layers<span class="token punctuation">.</span>common <span class="token keyword">import</span> RealVGGBlock<span class="token punctuation">,</span> LinearAddBlock
<span class="token keyword">def</span> <span class="token function">extract_blocks_into_list</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
   <span class="token keyword">for</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> LinearAddBlock<span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> RealVGGBlock<span class="token punctuation">)</span><span class="token punctuation">:</span>
            blocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>module<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            extract_blocks_into_list<span class="token punctuation">(</span>module<span class="token punctuation">,</span> blocks<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">RepVGGOptimizer</span><span class="token punctuation">(</span>SGD<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''scales is a list, scales[i] is a triple (scale_identity.weight, scale_1x1.weight, scale_conv.weight) or a two-tuple (scale_1x1.weight, scale_conv.weight) (if the block has no scale_identity)'''</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">,</span> scales<span class="token punctuation">,</span>
                 args<span class="token punctuation">,</span> cfg<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dampening<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> nesterov<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 reinit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> use_identity_scales_for_reinit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 cpu_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        defaults <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>lr<span class="token operator">=</span>cfg<span class="token punctuation">.</span>solver<span class="token punctuation">.</span>lr0<span class="token punctuation">,</span> momentum<span class="token operator">=</span>cfg<span class="token punctuation">.</span>solver<span class="token punctuation">.</span>momentum<span class="token punctuation">,</span> dampening<span class="token operator">=</span>dampening<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>weight_decay<span class="token punctuation">,</span> nesterov<span class="token operator">=</span>nesterov<span class="token punctuation">)</span>
        <span class="token keyword">if</span> nesterov <span class="token keyword">and</span> <span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>solver<span class="token punctuation">.</span>momentum <span class="token operator">&lt;=</span> <span class="token number">0</span> <span class="token keyword">or</span> dampening <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Nesterov momentum requires a momentum and zero dampening"</span><span class="token punctuation">)</span>
        <span class="token comment"># parameters = set_weight_decay(model)</span>
        parameters <span class="token operator">=</span> get_optimizer_param<span class="token punctuation">(</span>args<span class="token punctuation">,</span> cfg<span class="token punctuation">,</span> model<span class="token punctuation">)</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SGD<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>parameters<span class="token punctuation">,</span> defaults<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span>

        blocks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        extract_blocks_into_list<span class="token punctuation">(</span>model<span class="token punctuation">,</span> blocks<span class="token punctuation">)</span>
        convs <span class="token operator">=</span> <span class="token punctuation">[</span>b<span class="token punctuation">.</span>conv <span class="token keyword">for</span> b <span class="token keyword">in</span> blocks<span class="token punctuation">]</span>
        <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">len</span><span class="token punctuation">(</span>convs<span class="token punctuation">)</span>

        <span class="token keyword">if</span> reinit<span class="token punctuation">:</span>
            <span class="token keyword">for</span> m <span class="token keyword">in</span> model<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
                    gamma_init <span class="token operator">=</span> m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token keyword">if</span> gamma_init <span class="token operator">==</span> <span class="token number">1.0</span><span class="token punctuation">:</span>
                        LOGGER<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Checked. This is training from scratch.'</span><span class="token punctuation">)</span>
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        LOGGER<span class="token punctuation">.</span>warning<span class="token punctuation">(</span><span class="token string">'========================== Warning! Is this really training from scratch ? ================='</span><span class="token punctuation">)</span>
            LOGGER<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'##################### Re-initialize #############'</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>reinitialize<span class="token punctuation">(</span>scales<span class="token punctuation">,</span> convs<span class="token punctuation">,</span> use_identity_scales_for_reinit<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>generate_gradient_masks<span class="token punctuation">(</span>scales<span class="token punctuation">,</span> convs<span class="token punctuation">,</span> cpu_mode<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reinitialize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scales_by_idx<span class="token punctuation">,</span> conv3x3_by_idx<span class="token punctuation">,</span> use_identity_scales<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> scales<span class="token punctuation">,</span> conv3x3 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>scales_by_idx<span class="token punctuation">,</span> conv3x3_by_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_channels <span class="token operator">=</span> conv3x3<span class="token punctuation">.</span>in_channels
            out_channels <span class="token operator">=</span> conv3x3<span class="token punctuation">.</span>out_channels
            kernel_1x1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
                conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> conv3x3<span class="token punctuation">.</span>weight <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> \
                                      <span class="token operator">+</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>kernel_1x1<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span>
                <span class="token keyword">assert</span> in_channels <span class="token operator">==</span> out_channels
                identity <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np<span class="token punctuation">.</span>eye<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
                conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> conv3x3<span class="token punctuation">.</span>weight <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>kernel_1x1<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> use_identity_scales<span class="token punctuation">:</span>     <span class="token comment"># You may initialize the imaginary CSLA block with the trained identity_scale values. Makes almost no difference.</span>
                    identity_scale_weight <span class="token operator">=</span> scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                    conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">+=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>identity <span class="token operator">*</span> identity_scale_weight<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    conv3x3<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">+=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>identity<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">generate_gradient_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> scales_by_idx<span class="token punctuation">,</span> conv3x3_by_idx<span class="token punctuation">,</span> cpu_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>grad_mask_map <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
        <span class="token keyword">for</span> scales<span class="token punctuation">,</span> conv3x3 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>scales_by_idx<span class="token punctuation">,</span> conv3x3_by_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
            para <span class="token operator">=</span> conv3x3<span class="token punctuation">.</span>weight
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>scales<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
                mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>para<span class="token punctuation">,</span> device<span class="token operator">=</span>scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>scales<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>para<span class="token punctuation">,</span> device<span class="token operator">=</span>scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>scales<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> device<span class="token operator">=</span>scales<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>scales<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
                ids <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
                <span class="token keyword">assert</span> para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> para<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
                mask<span class="token punctuation">[</span>ids<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1.0</span>
            <span class="token keyword">if</span> cpu_mode<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>grad_mask_map<span class="token punctuation">[</span>para<span class="token punctuation">]</span> <span class="token operator">=</span> mask
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>grad_mask_map<span class="token punctuation">[</span>para<span class="token punctuation">]</span> <span class="token operator">=</span> mask<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__setstate__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SGD<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__setstate__<span class="token punctuation">(</span>state<span class="token punctuation">)</span>
        <span class="token keyword">for</span> group <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
            group<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span><span class="token string">'nesterov'</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> closure<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> closure <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            loss <span class="token operator">=</span> closure<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> group <span class="token keyword">in</span> self<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
            weight_decay <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'weight_decay'</span><span class="token punctuation">]</span>
            momentum <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'momentum'</span><span class="token punctuation">]</span>
            dampening <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'dampening'</span><span class="token punctuation">]</span>
            nesterov <span class="token operator">=</span> group<span class="token punctuation">[</span><span class="token string">'nesterov'</span><span class="token punctuation">]</span>

            <span class="token keyword">for</span> p <span class="token keyword">in</span> group<span class="token punctuation">[</span><span class="token string">'params'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                    <span class="token keyword">continue</span>

                <span class="token keyword">if</span> p <span class="token keyword">in</span> self<span class="token punctuation">.</span>grad_mask_map<span class="token punctuation">:</span>
                    d_p <span class="token operator">=</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data <span class="token operator">*</span> self<span class="token punctuation">.</span>grad_mask_map<span class="token punctuation">[</span>p<span class="token punctuation">]</span>  <span class="token comment"># Note: multiply the mask here</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    d_p <span class="token operator">=</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data

                <span class="token keyword">if</span> weight_decay <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    d_p<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>weight_decay<span class="token punctuation">,</span> p<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
                <span class="token keyword">if</span> momentum <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    param_state <span class="token operator">=</span> self<span class="token punctuation">.</span>state<span class="token punctuation">[</span>p<span class="token punctuation">]</span>
                    <span class="token keyword">if</span> <span class="token string">'momentum_buffer'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> param_state<span class="token punctuation">:</span>
                        buf <span class="token operator">=</span> param_state<span class="token punctuation">[</span><span class="token string">'momentum_buffer'</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>clone<span class="token punctuation">(</span>d_p<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        buf <span class="token operator">=</span> param_state<span class="token punctuation">[</span><span class="token string">'momentum_buffer'</span><span class="token punctuation">]</span>
                        buf<span class="token punctuation">.</span>mul_<span class="token punctuation">(</span>momentum<span class="token punctuation">)</span><span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> dampening<span class="token punctuation">,</span> d_p<span class="token punctuation">)</span>
                    <span class="token keyword">if</span> nesterov<span class="token punctuation">:</span>
                        d_p <span class="token operator">=</span> d_p<span class="token punctuation">.</span>add<span class="token punctuation">(</span>momentum<span class="token punctuation">,</span> buf<span class="token punctuation">)</span>
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        d_p <span class="token operator">=</span> buf

                p<span class="token punctuation">.</span>data<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token operator">-</span>group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d_p<span class="token punctuation">)</span>

        <span class="token keyword">return</span> loss
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

</li>
</ul>
<h3 id="Partial-quantization"><a href="#Partial-quantization" class="headerlink" title="Partial quantization"></a>Partial quantization</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/tools/partial_quantization/sensitivity_analyse.py">https://github.com/meituan/YOLOv6/blob/main/tools/partial_quantization/sensitivity_analyse.py</a></li>
</ul>
<blockquote>
<p>先做参数敏感性分析，再对不敏感的参数进行量化</p>
<p>所谓参数敏感性，就是判断参数变化对性能是否影响，如果影响很大，则敏感，否则不敏感, 采用控制变量法进行逐层分析</p>
<p>评价指标是在数据集上测试mAP，mAP相对全精度变化大则认为敏感</p>
<p>最后做敏感度排序</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">quant_sensitivity_analyse</span><span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> evaler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># disable all quantable layer</span>
    model_quant_disable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">)</span>

    <span class="token comment"># analyse each quantable layer</span>
    quant_sensitivity <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> k<span class="token punctuation">,</span> m <span class="token keyword">in</span> model_ptq<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>QuantConv2d<span class="token punctuation">)</span> <span class="token keyword">or</span> \
           <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>QuantConvTranspose2d<span class="token punctuation">)</span> <span class="token keyword">or</span> \
           <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            module_quant_enable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># module can not be quantized, continue</span>
            <span class="token keyword">continue</span>

        eval_result <span class="token operator">=</span> evaler<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span>model_ptq<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>eval_result<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Quantize Layer &#123;&#125;, result mAP0.5 = &#123;:0.4f&#125;, mAP0.5:0.95 = &#123;:0.4f&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span>
                                                                                          eval_result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                                                          eval_result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        quant_sensitivity<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>k<span class="token punctuation">,</span> eval_result<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> eval_result<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># disable this module sensitivity, anlayse next module</span>
        module_quant_disable<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">)</span>

    <span class="token keyword">return</span> quant_sensitivity<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">module_quant_disable</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cur_module <span class="token operator">=</span> get_module<span class="token punctuation">(</span>model<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>cur_module<span class="token punctuation">,</span> <span class="token string">'_input_quantizer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        cur_module<span class="token punctuation">.</span>_input_quantizer<span class="token punctuation">.</span>disable<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>cur_module<span class="token punctuation">,</span> <span class="token string">'_weight_quantizer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        cur_module<span class="token punctuation">.</span>_weight_quantizer<span class="token punctuation">.</span>disable<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">module_quant_enable</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span>
    cur_module <span class="token operator">=</span> get_module<span class="token punctuation">(</span>model<span class="token punctuation">,</span> k<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>cur_module<span class="token punctuation">,</span> <span class="token string">'_input_quantizer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        cur_module<span class="token punctuation">.</span>_input_quantizer<span class="token punctuation">.</span>enable<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>cur_module<span class="token punctuation">,</span> <span class="token string">'_weight_quantizer'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        cur_module<span class="token punctuation">.</span>_weight_quantizer<span class="token punctuation">.</span>enable<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>With partial quantization, we finally reach 42.1%, only 0.3% loss in accuracy, while the throughput of the partially quantized model is about 1.56 times that of the FP16 model at a batch size of 32. This method achieves a nice tradeoff between accuracy and throughput.</p>
</blockquote>
<h3 id="Pytorch-quantization"><a href="#Pytorch-quantization" class="headerlink" title="Pytorch quantization"></a>Pytorch quantization</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html</a></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Jermmy/pytorch-quantization-demo">https://github.com/Jermmy/pytorch-quantization-demo</a></li>
</ul>
</li>
<li><p>自定义量化模块 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/tutorials/creating_custom_quantized_modules.html">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/tutorials/creating_custom_quantized_modules.html</a></p>
<ul>
<li>主要是卷积层、反卷积层、池化层</li>
<li>注意输入或者权重的量化封装情况，有的只量化输入，有的要量化输入和权重</li>
<li>也可以自定义<code>TensorQuantizer</code>进行控制</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307281416590.png" alt="image-20230728141622388"></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/models/classification/resnet.py">https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/models/classification/resnet.py</a></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> quantize<span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> quant_nn<span class="token punctuation">.</span>QuantConv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>
                                self<span class="token punctuation">.</span>inplanes<span class="token punctuation">,</span>
                                kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
                                stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                                padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
                                bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>inplanes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Jermmy/pytorch-quantization-demo/blob/master/model.py#L34">https://github.com/Jermmy/pytorch-quantization-demo/blob/master/model.py#L34</a></p>
<p>  QAT需要对需要量化的层进行替换，比如QConv2d QReLU QMaxPooling2d QLinear，相当于用Q重构模型进行训练和推理，其中Qxx参考 <a target="_blank" rel="noopener" href="https://github.com/Jermmy/pytorch-quantization-demo/blob/master/module.py">https://github.com/Jermmy/pytorch-quantization-demo/blob/master/module.py</a> （最好用官方的进行替换QuantConv2d，或者源码替换QConv2d）</p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">40</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">quantize</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>qconv1 <span class="token operator">=</span> QConv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> qi<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qo<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_bits<span class="token operator">=</span>num_bits<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qrelu1 <span class="token operator">=</span> QReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qmaxpool2d_1 <span class="token operator">=</span> QMaxPooling2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qconv2 <span class="token operator">=</span> QConv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">,</span> qi<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> qo<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_bits<span class="token operator">=</span>num_bits<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qrelu2 <span class="token operator">=</span> QReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qmaxpool2d_2 <span class="token operator">=</span> QMaxPooling2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qfc <span class="token operator">=</span> QLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc<span class="token punctuation">,</span> qi<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> qo<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_bits<span class="token operator">=</span>num_bits<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">quantize_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qconv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qrelu1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qmaxpool2d_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qconv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qrelu2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qmaxpool2d_2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">40</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>qfc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">freeze</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qrelu1<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qmaxpool2d_1<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qconv2<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>qi<span class="token operator">=</span>self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qrelu2<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>qconv2<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qmaxpool2d_2<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>qconv2<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>qfc<span class="token punctuation">.</span>freeze<span class="token punctuation">(</span>qi<span class="token operator">=</span>self<span class="token punctuation">.</span>qconv2<span class="token punctuation">.</span>qo<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">quantize_inference</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>qi<span class="token punctuation">.</span>quantize_tensor<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qconv1<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qrelu1<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qmaxpool2d_1<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qconv2<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qrelu2<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qmaxpool2d_2<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        qx <span class="token operator">=</span> qx<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token operator">*</span><span class="token number">5</span><span class="token operator">*</span><span class="token number">40</span><span class="token punctuation">)</span>
        qx <span class="token operator">=</span> self<span class="token punctuation">.</span>qfc<span class="token punctuation">.</span>quantize_inference<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>qfc<span class="token punctuation">.</span>qo<span class="token punctuation">.</span>dequantize_tensor<span class="token punctuation">(</span>qx<span class="token punctuation">)</span>
        <span class="token keyword">return</span> out<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

</li>
</ul>
<h3 id="Post-training-quantization-PTQ"><a href="#Post-training-quantization-PTQ" class="headerlink" title="Post-training quantization (PTQ)"></a>Post-training quantization (PTQ)</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/tools/partial_quantization/ptq.py">https://github.com/meituan/YOLOv6/blob/main/tools/partial_quantization/ptq.py</a> </li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html#post-training-quantization">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html#post-training-quantization</a></li>
</ul>
<blockquote>
<p>QuantDescriptor 定义量化方法直方图，量化位数8bit，作为量化输入描述子</p>
<p>conv2d_weight_default_desc 作为权重量化描述子</p>
<p>针对Conv2d ConvTranspose2d MaxPool2d分别用相应的量化算子替代即可</p>
<p>PTQ只需迭代1-2epoch即可，而且是推理阶段（前向传播 with torch.no_grad()）</p>
<p>考虑敏感性分析，非敏感层可以用PTQ，敏感层不变，可以减少精度损失</p>
</blockquote>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">quant_model_init</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model_ptq <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>model<span class="token punctuation">)</span>
    model_ptq<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    model_ptq<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    conv2d_weight_default_desc <span class="token operator">=</span> tensor_quant<span class="token punctuation">.</span>QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL
    conv2d_input_default_desc <span class="token operator">=</span> QuantDescriptor<span class="token punctuation">(</span>num_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> calib_method<span class="token operator">=</span><span class="token string">'histogram'</span><span class="token punctuation">)</span>

    convtrans2d_weight_default_desc <span class="token operator">=</span> tensor_quant<span class="token punctuation">.</span>QUANT_DESC_8BIT_CONVTRANSPOSE2D_WEIGHT_PER_CHANNEL
    convtrans2d_input_default_desc <span class="token operator">=</span> QuantDescriptor<span class="token punctuation">(</span>num_bits<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> calib_method<span class="token operator">=</span><span class="token string">'histogram'</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> k<span class="token punctuation">,</span> m <span class="token keyword">in</span> model_ptq<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">'proj_conv'</span> <span class="token keyword">in</span> k<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Skip Layer &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">continue</span>

        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_channels <span class="token operator">=</span> m<span class="token punctuation">.</span>in_channels
            out_channels <span class="token operator">=</span> m<span class="token punctuation">.</span>out_channels
            kernel_size <span class="token operator">=</span> 
            m<span class="token punctuation">.</span>kernel_size
            stride <span class="token operator">=</span> m<span class="token punctuation">.</span>stride
            padding <span class="token operator">=</span> m<span class="token punctuation">.</span>padding
            quant_conv <span class="token operator">=</span> quant_nn<span class="token punctuation">.</span>QuantConv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>
                                              out_channels<span class="token punctuation">,</span>
                                              kernel_size<span class="token punctuation">,</span>
                                              stride<span class="token punctuation">,</span>
                                              padding<span class="token punctuation">,</span>
                                              quant_desc_input <span class="token operator">=</span> conv2d_input_default_desc<span class="token punctuation">,</span>
                                              quant_desc_weight <span class="token operator">=</span> conv2d_weight_default_desc<span class="token punctuation">)</span>
            quant_conv<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                quant_conv<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                quant_conv<span class="token punctuation">.</span>bias <span class="token operator">=</span> <span class="token boolean">None</span>
            set_module<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">,</span> quant_conv<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_channels <span class="token operator">=</span> m<span class="token punctuation">.</span>in_channels
            out_channels <span class="token operator">=</span> m<span class="token punctuation">.</span>out_channels
            kernel_size <span class="token operator">=</span> m<span class="token punctuation">.</span>kernel_size
            stride <span class="token operator">=</span> m<span class="token punctuation">.</span>stride
            padding <span class="token operator">=</span> m<span class="token punctuation">.</span>padding
            quant_convtrans <span class="token operator">=</span> quant_nn<span class="token punctuation">.</span>QuantConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span>
                                                       out_channels<span class="token punctuation">,</span>
                                                       kernel_size<span class="token punctuation">,</span>
                                                       stride<span class="token punctuation">,</span>
                                                       padding<span class="token punctuation">,</span>
                                                       quant_desc_input <span class="token operator">=</span> convtrans2d_input_default_desc<span class="token punctuation">,</span>
                                                       quant_desc_weight <span class="token operator">=</span> convtrans2d_weight_default_desc<span class="token punctuation">)</span>
            quant_convtrans<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                quant_convtrans<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                quant_convtrans<span class="token punctuation">.</span>bias <span class="token operator">=</span> <span class="token boolean">None</span>
            set_module<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">,</span> quant_convtrans<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
            kernel_size <span class="token operator">=</span> m<span class="token punctuation">.</span>kernel_size
            stride <span class="token operator">=</span> m<span class="token punctuation">.</span>stride
            padding <span class="token operator">=</span> m<span class="token punctuation">.</span>padding
            dilation <span class="token operator">=</span> m<span class="token punctuation">.</span>dilation
            ceil_mode <span class="token operator">=</span> m<span class="token punctuation">.</span>ceil_mode
            quant_maxpool2d <span class="token operator">=</span> quant_nn<span class="token punctuation">.</span>QuantMaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span>
                                                      stride<span class="token punctuation">,</span>
                                                      padding<span class="token punctuation">,</span>
                                                      dilation<span class="token punctuation">,</span>
                                                      ceil_mode<span class="token punctuation">,</span>
                                                      quant_desc_input <span class="token operator">=</span> conv2d_input_default_desc<span class="token punctuation">)</span>
            set_module<span class="token punctuation">(</span>model_ptq<span class="token punctuation">,</span> k<span class="token punctuation">,</span> quant_maxpool2d<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># module can not be quantized, continue</span>
            <span class="token keyword">continue</span>

    <span class="token keyword">return</span> model_ptq<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">collect_stats</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token triple-quoted-string string">"""Feed data to the network and collect statistic"""</span>

     <span class="token comment"># Enable calibrators</span>
     <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
         <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>TensorQuantizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
             <span class="token keyword">if</span> module<span class="token punctuation">.</span>_calibrator <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                 module<span class="token punctuation">.</span>disable_quant<span class="token punctuation">(</span><span class="token punctuation">)</span>
                 module<span class="token punctuation">.</span>enable_calib<span class="token punctuation">(</span><span class="token punctuation">)</span>
             <span class="token keyword">else</span><span class="token punctuation">:</span>
                 module<span class="token punctuation">.</span>disable<span class="token punctuation">(</span><span class="token punctuation">)</span>

     <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>image<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span><span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">,</span> total<span class="token operator">=</span>num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
         model<span class="token punctuation">(</span>image<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         <span class="token keyword">if</span> i <span class="token operator">>=</span> num_batches<span class="token punctuation">:</span>
             <span class="token keyword">break</span>

     <span class="token comment"># Disable calibrators</span>
     <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
         <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>TensorQuantizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
             <span class="token keyword">if</span> module<span class="token punctuation">.</span>_calibrator <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                 module<span class="token punctuation">.</span>enable_quant<span class="token punctuation">(</span><span class="token punctuation">)</span>
                 module<span class="token punctuation">.</span>disable_calib<span class="token punctuation">(</span><span class="token punctuation">)</span>
             <span class="token keyword">else</span><span class="token punctuation">:</span>
                 module<span class="token punctuation">.</span>enable<span class="token punctuation">(</span><span class="token punctuation">)</span>

 <span class="token keyword">def</span> <span class="token function">compute_amax</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token comment"># Load calib result</span>
     <span class="token keyword">for</span> name<span class="token punctuation">,</span> module <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
         <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> quant_nn<span class="token punctuation">.</span>TensorQuantizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
             <span class="token keyword">if</span> module<span class="token punctuation">.</span>_calibrator <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                 <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>module<span class="token punctuation">.</span>_calibrator<span class="token punctuation">,</span> calib<span class="token punctuation">.</span>MaxCalibrator<span class="token punctuation">)</span><span class="token punctuation">:</span>
                     module<span class="token punctuation">.</span>load_calib_amax<span class="token punctuation">(</span><span class="token punctuation">)</span>
                 <span class="token keyword">else</span><span class="token punctuation">:</span>
                     module<span class="token punctuation">.</span>load_calib_amax<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
             <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">F"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">:</span><span class="token format-spec">40</span><span class="token punctuation">&#125;</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>module<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
     model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># It is a bit slow since we collect histograms on CPU</span>
 <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     collect_stats<span class="token punctuation">(</span>model<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> num_batches<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
     compute_amax<span class="token punctuation">(</span>model<span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">"percentile"</span><span class="token punctuation">,</span> percentile<span class="token operator">=</span><span class="token number">99.99</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<blockquote>
<p>1 As of v0.2.0 release, traditional post-training quantization (PTQ) produces a degraded performance of <code>YOLOv6-S</code> from 43.4% to 41.2%. </p>
<p>​    直接使用PTQ，精度降低了2.2%</p>
<p>2 We apply post-training quantization to <code>YOLOv6-S-RepOpt</code>, and its mAP slightly drops by 0.5%. </p>
<p>​    使用了RepOPT、敏感性分析、PTQ之后，精度只降低了0.5%</p>
<p>3 Besides, we involve <strong>channel-wise distillation</strong> to accelerate the convergence. We finally reach a quantized model at 43.0% mAP.  The performance arrives at <strong>43.3% mAP</strong>, only 0.1% left to match the fully float precision of <code>YOLOv6-S</code>.</p>
<p>​    再使用QAT中通道蒸馏加快收敛，精度提高了0.4%，最后只损失了0.1%的精度</p>
</blockquote>
<h3 id="Quantization-aware-training-QAT"><a href="#Quantization-aware-training-QAT" class="headerlink" title="Quantization-aware training (QAT)"></a>Quantization-aware training (QAT)</h3><blockquote>
<p>1 Quantization Aware Training is based on Straight Through Estimator (STE) derivative approximation. </p>
<p>​    量化感知训练是基于直通式估算器（STE）导数优化</p>
<p>2 After calibration is done, Quantization Aware Training is simply select a training schedule and continue training the calibrated model. Usually, it doesn’t need to fine tune very long. We usually use around 10% of the original training schedule, starting at 1% of the initial training learning rate, and a cosine annealing learning rate schedule that follows the decreasing half of a cosine period, down to 1% of the initial fine tuning learning rate (0.01% of the initial training learning rate).</p>
<p>​    先标定int8，后QAT，二者需要互斥</p>
<p>​    在训练的时候大概10%的epochs，学习率要小，初始学习率占1%，用cosine annealing learning衰减，知道学习率衰减到0.01%的初始学习率</p>
<p>3 Do not change quantization representation (scale) during training, at least not too frequently. Changing scale every step</p>
<p>​    不要频繁的按照step改变学习率</p>
</blockquote>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/tools/qat/qat_utils.py">https://github.com/meituan/YOLOv6/blob/main/tools/qat/qat_utils.py</a></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html#quantization-aware-training">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html#quantization-aware-training</a></li>
</ul>
</li>
<li><p>通道蒸馏 channel-wise kd</p>
<ul>
<li><p>Channel-wise Knowledge Distillation for Dense Prediction</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.13256">https://arxiv.org/abs/2011.13256</a></li>
</ul>
</li>
<li><p>Spatial distillation：空间方向的蒸馏，可以理解成对所有通道的相同位置的点做归一化，然后让学生网络学习这个归一化后的分布，可以理解成对<strong>类别</strong>的蒸馏。</p>
</li>
<li><p>Channel distillation： 通道方向的蒸馏，可以理解成对单个通道内做归一化，然后让学生网络学习这个归一化后的分布，可以理解成对<strong>位置</strong>的蒸馏。</p>
</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149767.png" alt="image-20230731114708858"></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>distill<span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        t_preds<span class="token punctuation">,</span> t_featmaps <span class="token operator">=</span> self<span class="token punctuation">.</span>teacher_model<span class="token punctuation">(</span>images<span class="token punctuation">)</span>
    temperature <span class="token operator">=</span> self<span class="token punctuation">.</span>args<span class="token punctuation">.</span>temperature
    total_loss<span class="token punctuation">,</span> loss_items <span class="token operator">=</span> self<span class="token punctuation">.</span>compute_loss_distill<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> t_preds<span class="token punctuation">,</span> s_featmaps<span class="token punctuation">,</span> t_featmaps<span class="token punctuation">,</span> targets<span class="token punctuation">,</span> \
                                                      epoch_num<span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_epoch<span class="token punctuation">,</span> temperature<span class="token punctuation">,</span> step_num<span class="token punctuation">,</span>
                                                      batch_height<span class="token punctuation">,</span> batch_width<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">distill_loss_cw</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> s_feats<span class="token punctuation">,</span> t_feats<span class="token punctuation">,</span>  temperature<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token punctuation">,</span>W <span class="token operator">=</span> s_feats<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
    <span class="token comment"># print(N,C,H,W)</span>
    loss_cw <span class="token operator">=</span> F<span class="token punctuation">.</span>kl_div<span class="token punctuation">(</span>F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>s_feats<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>t_feats<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">,</span>
                       log_target<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>temperature <span class="token operator">*</span> temperature<span class="token punctuation">)</span><span class="token operator">/</span> <span class="token punctuation">(</span>N<span class="token operator">*</span>C<span class="token punctuation">)</span>

    N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token punctuation">,</span>W <span class="token operator">=</span> s_feats<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
    <span class="token comment"># print(N,C,H,W)</span>
    loss_cw <span class="token operator">+=</span> F<span class="token punctuation">.</span>kl_div<span class="token punctuation">(</span>F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>s_feats<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>t_feats<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">,</span>
                       log_target<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>temperature <span class="token operator">*</span> temperature<span class="token punctuation">)</span><span class="token operator">/</span> <span class="token punctuation">(</span>N<span class="token operator">*</span>C<span class="token punctuation">)</span>

    N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token punctuation">,</span>W <span class="token operator">=</span> s_feats<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape
    <span class="token comment"># print(N,C,H,W)</span>
    loss_cw <span class="token operator">+=</span> F<span class="token punctuation">.</span>kl_div<span class="token punctuation">(</span>F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>s_feats<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>t_feats<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>N<span class="token punctuation">,</span>C<span class="token punctuation">,</span>H<span class="token operator">*</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">/</span>temperature<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                       reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">,</span>
                       log_target<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>temperature <span class="token operator">*</span> temperature<span class="token punctuation">)</span><span class="token operator">/</span> <span class="token punctuation">(</span>N<span class="token operator">*</span>C<span class="token punctuation">)</span>
    <span class="token comment"># print(loss_cw)</span>
    <span class="token keyword">return</span> loss_cw<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h3 id="Export-to-ONNX"><a href="#Export-to-ONNX" class="headerlink" title="Export to ONNX"></a>Export to ONNX</h3><ul>
<li>使用了pytorch_quantization后模型由pt导出onnx需要使用pytorch的伪量化函数</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># First set static member of TensorQuantizer to use Pytorch’s own fake quantization functions</span>
<span class="token keyword">from</span> pytorch_quantization <span class="token keyword">import</span> nn <span class="token keyword">as</span> quant_nn
quant_nn<span class="token punctuation">.</span>TensorQuantizer<span class="token punctuation">.</span>use_fb_fake_quant <span class="token operator">=</span> <span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<h3 id="Deployment-with-TensorRT"><a href="#Deployment-with-TensorRT" class="headerlink" title="Deployment with TensorRT"></a>Deployment with TensorRT</h3><p>​    最好用docker-TensorRT-8.5版本</p>
<ul>
<li>performance</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307281448747.png" alt="image-20230728144808883"></p>
<ul>
<li>INT8 calibration</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307281508399.png" alt="image-20230728150155064"></p>
<ul>
<li>ablation study for yolov6-nano</li>
</ul>
<p><img src="https://raw.githubusercontent.com/lee-jet/Figure_Bed/main/images/202307311149768.png" alt="image-20230731102922901"></p>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ul>
<li>无论是PTQ推理量化，还是QAT训练量化，二者都需要对需要量化的层进行替换，最终保存是量化后网络的权重，而非网络结构</li>
<li>这篇文章工程能力很强，值得借鉴</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36051603">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md">https://github.com/meituan/YOLOv6/blob/main/docs/tutorial_repopt.md</a></li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">Lee Jet</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://lee-jet.github.io/2023/07/27/model-compression-and-acceleration-md/">https://lee-jet.github.io/2023/07/27/model-compression-and-acceleration-md/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">Lee Jet</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Tech-share/">
                                    <span class="chip bg-color">Tech share</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '239ba579785b9bb8ffde',
        clientSecret: '6b8159892d8d5785b7be9a84c0998462ce8649d4',
        repo: 'gitalk',
        owner: 'lee-jet',
        admin: "lee-jet",
        id: '2023-07-27T14-06-49',
        distractionFreeMode: false,  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/08/01/paper-reading/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/3.jpg" class="responsive-img" alt="paper-reading">
                        
                        <span class="card-title">paper-reading</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Paper-Reading/" class="post-category">
                                    Paper Reading
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Paper-Reading/">
                        <span class="chip bg-color">Paper Reading</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/02/22/baby-crying-detection-tutorials/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/15.jpg" class="responsive-img" alt="Baby Crying Recognition Tutorials of Algorithm Landing">
                        
                        <span class="card-title">Baby Crying Recognition Tutorials of Algorithm Landing</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-02-22
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Paper-Reading/" class="post-category">
                                    Paper Reading
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Paper-Reading/">
                        <span class="chip bg-color">Paper Reading</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('1'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023</span>
            
            <a href="/about" target="_blank">Lee Jet</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">75.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/lee-jet" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1874087716@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1874087716" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1874087716" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
     
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/libs/others/star.js"><\/script>');
            }
        </script>
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
